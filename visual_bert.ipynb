{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20149,"status":"ok","timestamp":1643299338083,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"6QRbVSKQuxMv","outputId":"d2589dd4-585f-46b9-d56a-5712fd92ef45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1643299338084,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"t7CYoEB0md5U","outputId":"e5dc9fb2-8559-40df-bab8-a498173e872c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/semeval-2022/maMi\n"]}],"source":["\n","%cd /content/drive/My Drive/semeval-2022/maMi"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14882,"status":"ok","timestamp":1643299352958,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"-vNYSCpuhfd4","outputId":"9cb4da71-bbc7-48e6-ebfc-0632567bb8d8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n","\u001b[K     |████████████████████████████████| 3.4 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 34.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 38.4 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 33.7 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bLGhl9m-uXZ8"},"outputs":[],"source":["##################################### VISUAL BERT ATTEMPT 2 ################"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1643299352959,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"1wPOuZ_Q8zLf"},"outputs":[],"source":["import random\n","\n","import numpy as np"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5666,"status":"ok","timestamp":1643299358617,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"SWaq1F5pGn1O","outputId":"170a8d51-1d83-4bd5-87b6-a104a0873dbb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=776fdb77c8c52029c053acc7be54302cd95bd0f7dc5980c05ab9e25034694229\n","  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"]}],"source":["!pip install wget"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7707,"status":"ok","timestamp":1643299366319,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"rfBljtdvnSPd"},"outputs":[],"source":["import sys\n","import numpy as np\n","import torch\n","from PIL import Image\n","from torch import nn\n","import copy\n","import fnmatch\n","import json\n","import os\n","import pickle as pkl\n","import shutil\n","import sys\n","import tarfile\n","import tempfile\n","from contextlib import contextmanager\n","from functools import partial\n","from hashlib import sha256\n","from io import BytesIO\n","from pathlib import Path\n","from urllib.parse import urlparse\n","from zipfile import ZipFile, is_zipfile\n","from tqdm.auto import tqdm\n","import cv2\n","import requests\n","import wget\n","from filelock import FileLock\n","from yaml import Loader, dump, load\n","\n","from abc import ABCMeta, abstractmethod\n","from collections import OrderedDict, namedtuple\n","from typing import Dict, List, Tuple\n","from torch.nn.modules.batchnorm import BatchNorm2d\n","from torchvision.ops import RoIPool\n","from torchvision.ops.boxes import batched_nms, nms"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1643299366320,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"cM3ZgJvjjsvC"},"outputs":[],"source":["random.seed(55)\n","np.random.seed(55)\n","torch.manual_seed(55)\n","torch.cuda.manual_seed_all(55)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1643299366320,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"1YU0uJiXnSRf"},"outputs":[],"source":["try:\n","    from torch.hub import _get_torch_home\n","\n","    torch_cache_home = _get_torch_home()\n","except ImportError:\n","    torch_cache_home = os.path.expanduser(\n","        os.getenv(\"TORCH_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"torch\"))\n","    )\n","\n","default_cache_path = os.path.join(torch_cache_home, \"transformers\")\n","CLOUDFRONT_DISTRIB_PREFIX = \"https://cdn.huggingface.co\"\n","S3_BUCKET_PREFIX = \"https://s3.amazonaws.com/models.huggingface.co/bert\"\n","#PATH = \"/\".join(str(Path(__file__).resolve()).split(\"/\")[:-1])\n","#CONFIG = os.path.join(PATH, \"config.yaml\")\n","#ATTRIBUTES = os.path.join(PATH, \"attributes.txt\")\n","#OBJECTS = os.path.join(PATH, \"objects.txt\")\n","PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\"PYTORCH_PRETRAINED_BERT_CACHE\", default_cache_path)\n","PYTORCH_TRANSFORMERS_CACHE = os.getenv(\"PYTORCH_TRANSFORMERS_CACHE\", PYTORCH_PRETRAINED_BERT_CACHE)\n","TRANSFORMERS_CACHE = os.getenv(\"TRANSFORMERS_CACHE\", PYTORCH_TRANSFORMERS_CACHE)\n","WEIGHTS_NAME = \"pytorch_model.bin\"\n","CONFIG_NAME = \"config.yaml\""]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2276,"status":"ok","timestamp":1643299368590,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"wTvLemc1g-Oj"},"outputs":[],"source":["'''def load_labels(objs=OBJECTS, attrs=ATTRIBUTES):\n","    vg_classes = []\n","    with open(objs) as f:\n","        for object in f.readlines():\n","            vg_classes.append(object.split(\",\")[0].lower().strip())\n","\n","    vg_attrs = []\n","    with open(attrs) as f:\n","        for object in f.readlines():\n","            vg_attrs.append(object.split(\",\")[0].lower().strip())\n","    return vg_classes, vg_attrs'''\n","\n","\n","def load_checkpoint(ckp):\n","    r = OrderedDict()\n","    with open(ckp, \"rb\") as f:\n","        ckp = pkl.load(f)[\"model\"]\n","    for k in copy.deepcopy(list(ckp.keys())):\n","        v = ckp.pop(k)\n","        if isinstance(v, np.ndarray):\n","            v = torch.tensor(v)\n","        else:\n","            assert isinstance(v, torch.tensor), type(v)\n","        r[k] = v\n","    return r\n","\n","\n","class Config:\n","    _pointer = {}\n","\n","    def __init__(self, dictionary: dict, name: str = \"root\", level=0):\n","        self._name = name\n","        self._level = level\n","        d = {}\n","        for k, v in dictionary.items():\n","            if v is None:\n","                raise ValueError()\n","            k = copy.deepcopy(k)\n","            v = copy.deepcopy(v)\n","            if isinstance(v, dict):\n","                v = Config(v, name=k, level=level + 1)\n","            d[k] = v\n","            setattr(self, k, v)\n","\n","        self._pointer = d\n","\n","    def __repr__(self):\n","        return str(list((self._pointer.keys())))\n","\n","    def __setattr__(self, key, val):\n","        self.__dict__[key] = val\n","        self.__dict__[key.upper()] = val\n","        levels = key.split(\".\")\n","        last_level = len(levels) - 1\n","        pointer = self._pointer\n","        if len(levels) > 1:\n","            for i, l in enumerate(levels):\n","                if hasattr(self, l) and isinstance(getattr(self, l), Config):\n","                    setattr(getattr(self, l), \".\".join(levels[i:]), val)\n","                if l == last_level:\n","                    pointer[l] = val\n","                else:\n","                    pointer = pointer[l]\n","\n","    def to_dict(self):\n","        return self._pointer\n","\n","    def dump_yaml(self, data, file_name):\n","        with open(f\"{file_name}\", \"w\") as stream:\n","            dump(data, stream)\n","\n","    def dump_json(self, data, file_name):\n","        with open(f\"{file_name}\", \"w\") as stream:\n","            json.dump(data, stream)\n","\n","    @staticmethod\n","    def load_yaml(config):\n","        with open(config) as stream:\n","            data = load(stream, Loader=Loader)\n","        return data\n","\n","    def __str__(self):\n","        t = \"    \"\n","        if self._name != \"root\":\n","            r = f\"{t * (self._level-1)}{self._name}:\\n\"\n","        else:\n","            r = \"\"\n","        level = self._level\n","        for i, (k, v) in enumerate(self._pointer.items()):\n","            if isinstance(v, Config):\n","                r += f\"{t * (self._level)}{v}\\n\"\n","                self._level += 1\n","            else:\n","                r += f\"{t * (self._level)}{k}: {v} ({type(v).__name__})\\n\"\n","            self._level = level\n","        return r[:-1]\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name_or_path: str, **kwargs):\n","        config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n","        return cls(config_dict)\n","\n","    @classmethod\n","    def get_config_dict(cls, pretrained_model_name_or_path: str, **kwargs):\n","\n","        cache_dir = kwargs.pop(\"cache_dir\", None)\n","        force_download = kwargs.pop(\"force_download\", False)\n","        resume_download = kwargs.pop(\"resume_download\", False)\n","        proxies = kwargs.pop(\"proxies\", None)\n","        local_files_only = kwargs.pop(\"local_files_only\", False)\n","\n","        if os.path.isdir(pretrained_model_name_or_path):\n","            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n","        elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n","            config_file = pretrained_model_name_or_path\n","        else:\n","            config_file = hf_bucket_url(pretrained_model_name_or_path, filename=CONFIG_NAME, use_cdn=False)\n","\n","        try:\n","            # Load from URL or cache if already cached\n","            resolved_config_file = cached_path(\n","                config_file,\n","                cache_dir=cache_dir,\n","                force_download=force_download,\n","                proxies=proxies,\n","                resume_download=resume_download,\n","                local_files_only=local_files_only,\n","            )\n","            # Load config dict\n","            if resolved_config_file is None:\n","                raise EnvironmentError\n","\n","            config_file = Config.load_yaml(resolved_config_file)\n","\n","        except EnvironmentError:\n","            msg = \"Can't load config for\"\n","            raise EnvironmentError(msg)\n","\n","        if resolved_config_file == config_file:\n","            print(\"loading configuration file from path\")\n","        else:\n","            print(\"loading configuration file cache\")\n","\n","        return Config.load_yaml(resolved_config_file), kwargs\n","\n","\n","# quick compare tensors\n","def compare(in_tensor):\n","\n","    out_tensor = torch.load(\"dump.pt\", map_location=in_tensor.device)\n","    n1 = in_tensor.numpy()\n","    n2 = out_tensor.numpy()[0]\n","    print(n1.shape, n1[0, 0, :5])\n","    print(n2.shape, n2[0, 0, :5])\n","    assert np.allclose(\n","        n1, n2, rtol=0.01, atol=0.1\n","    ), f\"{sum([1 for x in np.isclose(n1, n2, rtol=0.01, atol=0.1).flatten() if x == False])/len(n1.flatten())*100:.4f} % element-wise mismatch\"\n","    raise Exception(\"tensors are all good\")\n","\n","    # Hugging face functions below\n","\n","\n","def is_remote_url(url_or_filename):\n","    parsed = urlparse(url_or_filename)\n","    return parsed.scheme in (\"http\", \"https\")\n","\n","\n","def hf_bucket_url(model_id: str, filename: str, use_cdn=True) -> str:\n","    endpoint = CLOUDFRONT_DISTRIB_PREFIX if use_cdn else S3_BUCKET_PREFIX\n","    legacy_format = \"/\" not in model_id\n","    if legacy_format:\n","        return f\"{endpoint}/{model_id}-{filename}\"\n","    else:\n","        return f\"{endpoint}/{model_id}/{filename}\"\n","\n","\n","def http_get(\n","    url,\n","    temp_file,\n","    proxies=None,\n","    resume_size=0,\n","    user_agent=None,\n","):\n","    ua = \"python/{}\".format(sys.version.split()[0])\n","    ua += \"; torch/{}\".format(torch.__version__)\n","    if isinstance(user_agent, dict):\n","        ua += \"; \" + \"; \".join(\"{}/{}\".format(k, v) for k, v in user_agent.items())\n","    elif isinstance(user_agent, str):\n","        ua += \"; \" + user_agent\n","    headers = {\"user-agent\": ua}\n","    if resume_size > 0:\n","        headers[\"Range\"] = \"bytes=%d-\" % (resume_size,)\n","    response = requests.get(url, stream=True, proxies=proxies, headers=headers)\n","    if response.status_code == 416:  # Range not satisfiable\n","        return\n","    content_length = response.headers.get(\"Content-Length\")\n","    total = resume_size + int(content_length) if content_length is not None else None\n","    progress = tqdm(\n","        unit=\"B\",\n","        unit_scale=True,\n","        total=total,\n","        initial=resume_size,\n","        desc=\"Downloading\",\n","    )\n","    for chunk in response.iter_content(chunk_size=1024):\n","        if chunk:  # filter out keep-alive new chunks\n","            progress.update(len(chunk))\n","            temp_file.write(chunk)\n","    progress.close()\n","\n","\n","def get_from_cache(\n","    url,\n","    cache_dir=None,\n","    force_download=False,\n","    proxies=None,\n","    etag_timeout=10,\n","    resume_download=False,\n","    user_agent=None,\n","    local_files_only=False,\n","):\n","\n","    if cache_dir is None:\n","        cache_dir = TRANSFORMERS_CACHE\n","    if isinstance(cache_dir, Path):\n","        cache_dir = str(cache_dir)\n","\n","    os.makedirs(cache_dir, exist_ok=True)\n","\n","    etag = None\n","    if not local_files_only:\n","        try:\n","            response = requests.head(url, allow_redirects=True, proxies=proxies, timeout=etag_timeout)\n","            if response.status_code == 200:\n","                etag = response.headers.get(\"ETag\")\n","        except (EnvironmentError, requests.exceptions.Timeout):\n","            # etag is already None\n","            pass\n","\n","    filename = url_to_filename(url, etag)\n","\n","    # get cache path to put the file\n","    cache_path = os.path.join(cache_dir, filename)\n","\n","    # etag is None = we don't have a connection, or url doesn't exist, or is otherwise inaccessible.\n","    # try to get the last downloaded one\n","    if etag is None:\n","        if os.path.exists(cache_path):\n","            return cache_path\n","        else:\n","            matching_files = [\n","                file\n","                for file in fnmatch.filter(os.listdir(cache_dir), filename + \".*\")\n","                if not file.endswith(\".json\") and not file.endswith(\".lock\")\n","            ]\n","            if len(matching_files) > 0:\n","                return os.path.join(cache_dir, matching_files[-1])\n","            else:\n","                # If files cannot be found and local_files_only=True,\n","                # the models might've been found if local_files_only=False\n","                # Notify the user about that\n","                if local_files_only:\n","                    raise ValueError(\n","                        \"Cannot find the requested files in the cached path and outgoing traffic has been\"\n","                        \" disabled. To enable model look-ups and downloads online, set 'local_files_only'\"\n","                        \" to False.\"\n","                    )\n","                return None\n","\n","    # From now on, etag is not None.\n","    if os.path.exists(cache_path) and not force_download:\n","        return cache_path\n","\n","    # Prevent parallel downloads of the same file with a lock.\n","    lock_path = cache_path + \".lock\"\n","    with FileLock(lock_path):\n","\n","        # If the download just completed while the lock was activated.\n","        if os.path.exists(cache_path) and not force_download:\n","            # Even if returning early like here, the lock will be released.\n","            return cache_path\n","\n","        if resume_download:\n","            incomplete_path = cache_path + \".incomplete\"\n","\n","            @contextmanager\n","            def _resumable_file_manager():\n","                with open(incomplete_path, \"a+b\") as f:\n","                    yield f\n","\n","            temp_file_manager = _resumable_file_manager\n","            if os.path.exists(incomplete_path):\n","                resume_size = os.stat(incomplete_path).st_size\n","            else:\n","                resume_size = 0\n","        else:\n","            temp_file_manager = partial(tempfile.NamedTemporaryFile, dir=cache_dir, delete=False)\n","            resume_size = 0\n","\n","        # Download to temporary file, then copy to cache dir once finished.\n","        # Otherwise you get corrupt cache entries if the download gets interrupted.\n","        with temp_file_manager() as temp_file:\n","            print(\n","                \"%s not found in cache or force_download set to True, downloading to %s\",\n","                url,\n","                temp_file.name,\n","            )\n","\n","            http_get(\n","                url,\n","                temp_file,\n","                proxies=proxies,\n","                resume_size=resume_size,\n","                user_agent=user_agent,\n","            )\n","\n","        os.replace(temp_file.name, cache_path)\n","\n","        meta = {\"url\": url, \"etag\": etag}\n","        meta_path = cache_path + \".json\"\n","        with open(meta_path, \"w\") as meta_file:\n","            json.dump(meta, meta_file)\n","\n","    return cache_path\n","\n","\n","def url_to_filename(url, etag=None):\n","\n","    url_bytes = url.encode(\"utf-8\")\n","    url_hash = sha256(url_bytes)\n","    filename = url_hash.hexdigest()\n","\n","    if etag:\n","        etag_bytes = etag.encode(\"utf-8\")\n","        etag_hash = sha256(etag_bytes)\n","        filename += \".\" + etag_hash.hexdigest()\n","\n","    if url.endswith(\".h5\"):\n","        filename += \".h5\"\n","\n","    return filename\n","\n","\n","def cached_path(\n","    url_or_filename,\n","    cache_dir=None,\n","    force_download=False,\n","    proxies=None,\n","    resume_download=False,\n","    user_agent=None,\n","    extract_compressed_file=False,\n","    force_extract=False,\n","    local_files_only=False,\n","):\n","    if cache_dir is None:\n","        cache_dir = TRANSFORMERS_CACHE\n","    if isinstance(url_or_filename, Path):\n","        url_or_filename = str(url_or_filename)\n","    if isinstance(cache_dir, Path):\n","        cache_dir = str(cache_dir)\n","\n","    if is_remote_url(url_or_filename):\n","        # URL, so get it from the cache (downloading if necessary)\n","        output_path = get_from_cache(\n","            url_or_filename,\n","            cache_dir=cache_dir,\n","            force_download=force_download,\n","            proxies=proxies,\n","            resume_download=resume_download,\n","            user_agent=user_agent,\n","            local_files_only=local_files_only,\n","        )\n","    elif os.path.exists(url_or_filename):\n","        # File, and it exists.\n","        output_path = url_or_filename\n","    elif urlparse(url_or_filename).scheme == \"\":\n","        # File, but it doesn't exist.\n","        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n","    else:\n","        # Something unknown\n","        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n","\n","    if extract_compressed_file:\n","        if not is_zipfile(output_path) and not tarfile.is_tarfile(output_path):\n","            return output_path\n","\n","        # Path where we extract compressed archives\n","        # We avoid '.' in dir name and add \"-extracted\" at the end: \"./model.zip\" => \"./model-zip-extracted/\"\n","        output_dir, output_file = os.path.split(output_path)\n","        output_extract_dir_name = output_file.replace(\".\", \"-\") + \"-extracted\"\n","        output_path_extracted = os.path.join(output_dir, output_extract_dir_name)\n","\n","        if os.path.isdir(output_path_extracted) and os.listdir(output_path_extracted) and not force_extract:\n","            return output_path_extracted\n","\n","        # Prevent parallel extractions\n","        lock_path = output_path + \".lock\"\n","        with FileLock(lock_path):\n","            shutil.rmtree(output_path_extracted, ignore_errors=True)\n","            os.makedirs(output_path_extracted)\n","            if is_zipfile(output_path):\n","                with ZipFile(output_path, \"r\") as zip_file:\n","                    zip_file.extractall(output_path_extracted)\n","                    zip_file.close()\n","            elif tarfile.is_tarfile(output_path):\n","                tar_file = tarfile.open(output_path)\n","                tar_file.extractall(output_path_extracted)\n","                tar_file.close()\n","            else:\n","                raise EnvironmentError(\"Archive format of {} could not be identified\".format(output_path))\n","\n","        return output_path_extracted\n","\n","    return output_path\n","\n","\n","def get_data(query, delim=\",\"):\n","    assert isinstance(query, str)\n","    if os.path.isfile(query):\n","        with open(query) as f:\n","            data = eval(f.read())\n","    else:\n","        req = requests.get(query)\n","        try:\n","            data = requests.json()\n","        except Exception:\n","            data = req.content.decode()\n","            assert data is not None, \"could not connect\"\n","            try:\n","                data = eval(data)\n","            except Exception:\n","                data = data.split(\"\\n\")\n","        req.close()\n","    return data\n","\n","\n","def get_image_from_url(url):\n","    response = requests.get(url)\n","    img = np.array(Image.open(BytesIO(response.content)))\n","    return img\n","\n","\n","# to load legacy frcnn checkpoint from detectron\n","def load_frcnn_pkl_from_url(url):\n","    fn = url.split(\"/\")[-1]\n","    if fn not in os.listdir(os.getcwd()):\n","        wget.download(url)\n","    with open(fn, \"rb\") as stream:\n","        weights = pkl.load(stream)\n","    model = weights.pop(\"model\")\n","    new = {}\n","    for k, v in model.items():\n","        new[k] = torch.from_numpy(v)\n","        if \"running_var\" in k:\n","            zero = torch.tensor([0])\n","            k2 = k.replace(\"running_var\", \"num_batches_tracked\")\n","            new[k2] = zero\n","    return new\n","\n","\n","def get_demo_path():\n","    print(f\"{os.path.abspath(os.path.join(PATH, os.pardir))}/demo.ipynb\")\n","\n","\n","def img_tensorize(im, input_format=\"RGB\"):\n","    assert isinstance(im, str)\n","    if os.path.isfile(im):\n","        img = cv2.imread(im)\n","    else:\n","        img = get_image_from_url(im)\n","        assert img is not None, f\"could not connect to: {im}\"\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    if input_format == \"RGB\":\n","        img = img[:, :, ::-1]\n","    return img\n","\n","\n","def chunk(images, batch=1):\n","    return (images[i : i + batch] for i in range(0, len(images), batch))"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":816,"status":"ok","timestamp":1643299369403,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"b3kuuS3znSXI"},"outputs":[],"source":["class ResizeShortestEdge:\n","    def __init__(self, short_edge_length, max_size=sys.maxsize):\n","        \"\"\"\n","        Args:\n","            short_edge_length (list[min, max])\n","            max_size (int): maximum allowed longest edge length.\n","        \"\"\"\n","        self.interp_method = \"bilinear\"\n","        self.max_size = max_size\n","        self.short_edge_length = short_edge_length\n","\n","    def __call__(self, imgs):\n","        img_augs = []\n","        for img in imgs:\n","            h, w = img.shape[:2]\n","            # later: provide list and randomly choose index for resize\n","            size = np.random.randint(self.short_edge_length[0], self.short_edge_length[1] + 1)\n","            if size == 0:\n","                return img\n","            scale = size * 1.0 / min(h, w)\n","            if h < w:\n","                newh, neww = size, scale * w\n","            else:\n","                newh, neww = scale * h, size\n","            if max(newh, neww) > self.max_size:\n","                scale = self.max_size * 1.0 / max(newh, neww)\n","                newh = newh * scale\n","                neww = neww * scale\n","            neww = int(neww + 0.5)\n","            newh = int(newh + 0.5)\n","\n","            if img.dtype == np.uint8:\n","                pil_image = Image.fromarray(img)\n","                pil_image = pil_image.resize((neww, newh), Image.BILINEAR)\n","                img = np.asarray(pil_image)\n","            else:\n","                img = img.permute(2, 0, 1).unsqueeze(0)  # 3, 0, 1)  # hw(c) -> nchw\n","                img = nn.functional.interpolate(\n","                    img, (newh, neww), mode=self.interp_method, align_corners=False\n","                ).squeeze(0)\n","            img_augs.append(img)\n","\n","        return img_augs\n","\n","\n","class Preprocess:\n","    def __init__(self, cfg):\n","        self.aug = ResizeShortestEdge([cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST)\n","        self.input_format = cfg.INPUT.FORMAT\n","        self.size_divisibility = cfg.SIZE_DIVISIBILITY\n","        self.pad_value = cfg.PAD_VALUE\n","        self.max_image_size = cfg.INPUT.MAX_SIZE_TEST\n","        self.device = cfg.MODEL.DEVICE\n","        self.pixel_std = torch.tensor(cfg.MODEL.PIXEL_STD).to(self.device).view(len(cfg.MODEL.PIXEL_STD), 1, 1)\n","        self.pixel_mean = torch.tensor(cfg.MODEL.PIXEL_MEAN).to(self.device).view(len(cfg.MODEL.PIXEL_STD), 1, 1)\n","        self.normalizer = lambda x: (x - self.pixel_mean) / self.pixel_std\n","\n","    def pad(self, images):\n","        max_size = tuple(max(s) for s in zip(*[img.shape for img in images]))\n","        image_sizes = [im.shape[-2:] for im in images]\n","        images = [\n","            nn.functional.pad(\n","                im,\n","                [0, max_size[-1] - size[1], 0, max_size[-2] - size[0]],\n","                value=self.pad_value,\n","            )\n","            for size, im in zip(image_sizes, images)\n","        ]\n","\n","        return torch.stack(images), torch.tensor(image_sizes)\n","\n","    def __call__(self, images, single_image=False):\n","        with torch.no_grad():\n","            if not isinstance(images, list):\n","                images = [images]\n","            if single_image:\n","                assert len(images) == 1\n","            for i in range(len(images)):\n","                if isinstance(images[i], torch.Tensor):\n","                    images.insert(i, images.pop(i).to(self.device).float())\n","                elif not isinstance(images[i], torch.Tensor):\n","                    images.insert(\n","                        i,\n","                        torch.as_tensor(img_tensorize(images.pop(i), input_format=self.input_format))\n","                        .to(self.device)\n","                        .float(),\n","                    )\n","            # resize smallest edge\n","            raw_sizes = torch.tensor([im.shape[:2] for im in images])\n","            images = self.aug(images)\n","            # transpose images and convert to torch tensors\n","            # images = [torch.as_tensor(i.astype(\"float32\")).permute(2, 0, 1).to(self.device) for i in images]\n","            # now normalize before pad to avoid useless arithmetic\n","            images = [self.normalizer(x) for x in images]\n","            # now pad them to do the following operations\n","            images, sizes = self.pad(images)\n","            # Normalize\n","\n","            if self.size_divisibility > 0:\n","                raise NotImplementedError()\n","            # pad\n","            scales_yx = torch.true_divide(raw_sizes, sizes)\n","            if single_image:\n","                return images[0], sizes[0], scales_yx[0]\n","            else:\n","                return images, sizes, scales_yx\n","\n","\n","def _scale_box(boxes, scale_yx):\n","    boxes[:, 0::2] *= scale_yx[:, 1]\n","    boxes[:, 1::2] *= scale_yx[:, 0]\n","    return boxes\n","\n","\n","def _clip_box(tensor, box_size: Tuple[int, int]):\n","    assert torch.isfinite(tensor).all(), \"Box tensor contains infinite or NaN!\"\n","    h, w = box_size\n","    tensor[:, 0].clamp_(min=0, max=w)\n","    tensor[:, 1].clamp_(min=0, max=h)\n","    tensor[:, 2].clamp_(min=0, max=w)\n","    tensor[:, 3].clamp_(min=0, max=h)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":51765,"status":"ok","timestamp":1643299421166,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"RVinKTWwnSa5"},"outputs":[],"source":["# other:\n","def norm_box(boxes, raw_sizes):\n","    if not isinstance(boxes, torch.Tensor):\n","        normalized_boxes = boxes.copy()\n","    else:\n","        normalized_boxes = boxes.clone()\n","    normalized_boxes[:, :, (0, 2)] /= raw_sizes[:, 1]\n","    normalized_boxes[:, :, (1, 3)] /= raw_sizes[:, 0]\n","    return normalized_boxes\n","\n","\n","def pad_list_tensors(\n","    list_tensors,\n","    preds_per_image,\n","    max_detections=None,\n","    return_tensors=None,\n","    padding=None,\n","    pad_value=0,\n","    location=None,\n","):\n","    \"\"\"\n","    location will always be cpu for np tensors\n","    \"\"\"\n","    if location is None:\n","        location = \"cpu\"\n","    assert return_tensors in {\"pt\", \"np\", None}\n","    assert padding in {\"max_detections\", \"max_batch\", None}\n","    new = []\n","    if padding is None:\n","        if return_tensors is None:\n","            return list_tensors\n","        elif return_tensors == \"pt\":\n","            if not isinstance(list_tensors, torch.Tensor):\n","                return torch.stack(list_tensors).to(location)\n","            else:\n","                return list_tensors.to(location)\n","        else:\n","            if not isinstance(list_tensors, list):\n","                return np.array(list_tensors.to(location))\n","            else:\n","                return list_tensors.to(location)\n","    if padding == \"max_detections\":\n","        assert max_detections is not None, \"specify max number of detections per batch\"\n","    elif padding == \"max_batch\":\n","        max_detections = max(preds_per_image)\n","    for i in range(len(list_tensors)):\n","        too_small = False\n","        tensor_i = list_tensors.pop(0)\n","        if tensor_i.ndim < 2:\n","            too_small = True\n","            tensor_i = tensor_i.unsqueeze(-1)\n","        assert isinstance(tensor_i, torch.Tensor)\n","        tensor_i = nn.functional.pad(\n","            input=tensor_i,\n","            pad=(0, 0, 0, max_detections - preds_per_image[i]),\n","            mode=\"constant\",\n","            value=pad_value,\n","        )\n","        if too_small:\n","            tensor_i = tensor_i.squeeze(-1)\n","        if return_tensors is None:\n","            if location == \"cpu\":\n","                tensor_i = tensor_i.cpu()\n","            tensor_i = tensor_i.tolist()\n","        if return_tensors == \"np\":\n","            if location == \"cpu\":\n","                tensor_i = tensor_i.cpu()\n","            tensor_i = tensor_i.numpy()\n","        else:\n","            if location == \"cpu\":\n","                tensor_i = tensor_i.cpu()\n","        new.append(tensor_i)\n","    if return_tensors == \"np\":\n","        return np.stack(new, axis=0)\n","    elif return_tensors == \"pt\" and not isinstance(new, torch.Tensor):\n","        return torch.stack(new, dim=0)\n","    else:\n","        return list_tensors\n","\n","\n","def do_nms(boxes, scores, image_shape, score_thresh, nms_thresh, mind, maxd):\n","    scores = scores[:, :-1]\n","    num_bbox_reg_classes = boxes.shape[1] // 4\n","    # Convert to Boxes to use the `clip` function ...\n","    boxes = boxes.reshape(-1, 4)\n","    _clip_box(boxes, image_shape)\n","    boxes = boxes.view(-1, num_bbox_reg_classes, 4)  # R x C x 4\n","\n","    # Select max scores\n","    max_scores, max_classes = scores.max(1)  # R x C --> R\n","    num_objs = boxes.size(0)\n","    boxes = boxes.view(-1, 4)\n","    idxs = torch.arange(num_objs).to(boxes.device) * num_bbox_reg_classes + max_classes\n","    max_boxes = boxes[idxs]  # Select max boxes according to the max scores.\n","\n","    # Apply NMS\n","    keep = nms(max_boxes, max_scores, nms_thresh)\n","    keep = keep[:maxd]\n","    if keep.shape[-1] >= mind and keep.shape[-1] <= maxd:\n","        max_boxes, max_scores = max_boxes[keep], max_scores[keep]\n","        classes = max_classes[keep]\n","        return max_boxes, max_scores, classes, keep\n","    else:\n","        return None\n","\n","\n","# Helper Functions\n","def _clip_box(tensor, box_size: Tuple[int, int]):\n","    assert torch.isfinite(tensor).all(), \"Box tensor contains infinite or NaN!\"\n","    h, w = box_size\n","    tensor[:, 0].clamp_(min=0, max=w)\n","    tensor[:, 1].clamp_(min=0, max=h)\n","    tensor[:, 2].clamp_(min=0, max=w)\n","    tensor[:, 3].clamp_(min=0, max=h)\n","\n","\n","def _nonempty_boxes(box, threshold: float = 0.0) -> torch.Tensor:\n","    widths = box[:, 2] - box[:, 0]\n","    heights = box[:, 3] - box[:, 1]\n","    keep = (widths > threshold) & (heights > threshold)\n","    return keep\n","\n","\n","def get_norm(norm, out_channels):\n","    if isinstance(norm, str):\n","        if len(norm) == 0:\n","            return None\n","        norm = {\n","            \"BN\": BatchNorm2d,\n","            \"GN\": lambda channels: nn.GroupNorm(32, channels),\n","            \"nnSyncBN\": nn.SyncBatchNorm,  # keep for debugging\n","            \"\": lambda x: x,\n","        }[norm]\n","    return norm(out_channels)\n","\n","\n","def _create_grid_offsets(size: List[int], stride: int, offset: float, device):\n","\n","    grid_height, grid_width = size\n","    shifts_x = torch.arange(\n","        offset * stride,\n","        grid_width * stride,\n","        step=stride,\n","        dtype=torch.float32,\n","        device=device,\n","    )\n","    shifts_y = torch.arange(\n","        offset * stride,\n","        grid_height * stride,\n","        step=stride,\n","        dtype=torch.float32,\n","        device=device,\n","    )\n","\n","    shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\n","    shift_x = shift_x.reshape(-1)\n","    shift_y = shift_y.reshape(-1)\n","    return shift_x, shift_y\n","\n","\n","def build_backbone(cfg):\n","    input_shape = ShapeSpec(channels=len(cfg.MODEL.PIXEL_MEAN))\n","    norm = cfg.RESNETS.NORM\n","    stem = BasicStem(\n","        in_channels=input_shape.channels,\n","        out_channels=cfg.RESNETS.STEM_OUT_CHANNELS,\n","        norm=norm,\n","        caffe_maxpool=cfg.MODEL.MAX_POOL,\n","    )\n","    freeze_at = cfg.BACKBONE.FREEZE_AT\n","\n","    if freeze_at >= 1:\n","        for p in stem.parameters():\n","            p.requires_grad = False\n","\n","    out_features = cfg.RESNETS.OUT_FEATURES\n","    depth = cfg.RESNETS.DEPTH\n","    num_groups = cfg.RESNETS.NUM_GROUPS\n","    width_per_group = cfg.RESNETS.WIDTH_PER_GROUP\n","    bottleneck_channels = num_groups * width_per_group\n","    in_channels = cfg.RESNETS.STEM_OUT_CHANNELS\n","    out_channels = cfg.RESNETS.RES2_OUT_CHANNELS\n","    stride_in_1x1 = cfg.RESNETS.STRIDE_IN_1X1\n","    res5_dilation = cfg.RESNETS.RES5_DILATION\n","    assert res5_dilation in {1, 2}, \"res5_dilation cannot be {}.\".format(res5_dilation)\n","\n","    num_blocks_per_stage = {50: [3, 4, 6, 3], 101: [3, 4, 23, 3], 152: [3, 8, 36, 3]}[depth]\n","\n","    stages = []\n","    out_stage_idx = [{\"res2\": 2, \"res3\": 3, \"res4\": 4, \"res5\": 5}[f] for f in out_features]\n","    max_stage_idx = max(out_stage_idx)\n","    for idx, stage_idx in enumerate(range(2, max_stage_idx + 1)):\n","        dilation = res5_dilation if stage_idx == 5 else 1\n","        first_stride = 1 if idx == 0 or (stage_idx == 5 and dilation == 2) else 2\n","        stage_kargs = {\n","            \"num_blocks\": num_blocks_per_stage[idx],\n","            \"first_stride\": first_stride,\n","            \"in_channels\": in_channels,\n","            \"bottleneck_channels\": bottleneck_channels,\n","            \"out_channels\": out_channels,\n","            \"num_groups\": num_groups,\n","            \"norm\": norm,\n","            \"stride_in_1x1\": stride_in_1x1,\n","            \"dilation\": dilation,\n","        }\n","\n","        stage_kargs[\"block_class\"] = BottleneckBlock\n","        blocks = ResNet.make_stage(**stage_kargs)\n","        in_channels = out_channels\n","        out_channels *= 2\n","        bottleneck_channels *= 2\n","\n","        if freeze_at >= stage_idx:\n","            for block in blocks:\n","                block.freeze()\n","        stages.append(blocks)\n","\n","    return ResNet(stem, stages, out_features=out_features)\n","\n","\n","def find_top_rpn_proposals(\n","    proposals,\n","    pred_objectness_logits,\n","    images,\n","    image_sizes,\n","    nms_thresh,\n","    pre_nms_topk,\n","    post_nms_topk,\n","    min_box_side_len,\n","    training,\n","):\n","    \"\"\"Args:\n","        proposals (list[Tensor]): (L, N, Hi*Wi*A, 4).\n","        pred_objectness_logits: tensors of length L.\n","        nms_thresh (float): IoU threshold to use for NMS\n","        pre_nms_topk (int): before nms\n","        post_nms_topk (int): after nms\n","        min_box_side_len (float): minimum proposal box side\n","        training (bool): True if proposals are to be used in training,\n","    Returns:\n","        results (List[Dict]): stores post_nms_topk object proposals for image i.\n","    \"\"\"\n","    num_images = len(images)\n","    device = proposals[0].device\n","\n","    # 1. Select top-k anchor for every level and every image\n","    topk_scores = []  # #lvl Tensor, each of shape N x topk\n","    topk_proposals = []\n","    level_ids = []  # #lvl Tensor, each of shape (topk,)\n","    batch_idx = torch.arange(num_images, device=device)\n","    for level_id, proposals_i, logits_i in zip(itertools.count(), proposals, pred_objectness_logits):\n","        Hi_Wi_A = logits_i.shape[1]\n","        num_proposals_i = min(pre_nms_topk, Hi_Wi_A)\n","\n","        # sort is faster than topk (https://github.com/pytorch/pytorch/issues/22812)\n","        # topk_scores_i, topk_idx = logits_i.topk(num_proposals_i, dim=1)\n","        logits_i, idx = logits_i.sort(descending=True, dim=1)\n","        topk_scores_i = logits_i[batch_idx, :num_proposals_i]\n","        topk_idx = idx[batch_idx, :num_proposals_i]\n","\n","        # each is N x topk\n","        topk_proposals_i = proposals_i[batch_idx[:, None], topk_idx]  # N x topk x 4\n","\n","        topk_proposals.append(topk_proposals_i)\n","        topk_scores.append(topk_scores_i)\n","        level_ids.append(torch.full((num_proposals_i,), level_id, dtype=torch.int64, device=device))\n","\n","    # 2. Concat all levels together\n","    topk_scores = torch.cat(topk_scores, dim=1)\n","    topk_proposals = torch.cat(topk_proposals, dim=1)\n","    level_ids = torch.cat(level_ids, dim=0)\n","\n","    # if I change to batched_nms, I wonder if this will make a difference\n","    # 3. For each image, run a per-level NMS, and choose topk results.\n","    results = []\n","    for n, image_size in enumerate(image_sizes):\n","        boxes = topk_proposals[n]\n","        scores_per_img = topk_scores[n]\n","        # I will have to take a look at the boxes clip method\n","        _clip_box(boxes, image_size)\n","        # filter empty boxes\n","        keep = _nonempty_boxes(boxes, threshold=min_box_side_len)\n","        lvl = level_ids\n","        if keep.sum().item() != len(boxes):\n","            boxes, scores_per_img, lvl = (\n","                boxes[keep],\n","                scores_per_img[keep],\n","                level_ids[keep],\n","            )\n","\n","        keep = batched_nms(boxes, scores_per_img, lvl, nms_thresh)\n","        keep = keep[:post_nms_topk]\n","\n","        res = (boxes[keep], scores_per_img[keep])\n","        results.append(res)\n","\n","    # I wonder if it would be possible for me to pad all these things.\n","    return results\n","\n","\n","def subsample_labels(labels, num_samples, positive_fraction, bg_label):\n","    \"\"\"\n","    Returns:\n","        pos_idx, neg_idx (Tensor):\n","            1D vector of indices. The total length of both is `num_samples` or fewer.\n","    \"\"\"\n","    positive = torch.nonzero((labels != -1) & (labels != bg_label)).squeeze(1)\n","    negative = torch.nonzero(labels == bg_label).squeeze(1)\n","\n","    num_pos = int(num_samples * positive_fraction)\n","    # protect against not enough positive examples\n","    num_pos = min(positive.numel(), num_pos)\n","    num_neg = num_samples - num_pos\n","    # protect against not enough negative examples\n","    num_neg = min(negative.numel(), num_neg)\n","\n","    # randomly select positive and negative examples\n","    perm1 = torch.randperm(positive.numel(), device=positive.device)[:num_pos]\n","    perm2 = torch.randperm(negative.numel(), device=negative.device)[:num_neg]\n","\n","    pos_idx = positive[perm1]\n","    neg_idx = negative[perm2]\n","    return pos_idx, neg_idx\n","\n","\n","def add_ground_truth_to_proposals(gt_boxes, proposals):\n","    raise NotImplementedError()\n","\n","\n","def add_ground_truth_to_proposals_single_image(gt_boxes, proposals):\n","    raise NotImplementedError()\n","\n","\n","def _fmt_box_list(box_tensor, batch_index: int):\n","    repeated_index = torch.full(\n","        (len(box_tensor), 1),\n","        batch_index,\n","        dtype=box_tensor.dtype,\n","        device=box_tensor.device,\n","    )\n","    return torch.cat((repeated_index, box_tensor), dim=1)\n","\n","\n","def convert_boxes_to_pooler_format(box_lists: List[torch.Tensor]):\n","    pooler_fmt_boxes = torch.cat(\n","        [_fmt_box_list(box_list, i) for i, box_list in enumerate(box_lists)],\n","        dim=0,\n","    )\n","    return pooler_fmt_boxes\n","\n","\n","def assign_boxes_to_levels(\n","    box_lists: List[torch.Tensor],\n","    min_level: int,\n","    max_level: int,\n","    canonical_box_size: int,\n","    canonical_level: int,\n","):\n","\n","    box_sizes = torch.sqrt(torch.cat([boxes.area() for boxes in box_lists]))\n","    # Eqn.(1) in FPN paper\n","    level_assignments = torch.floor(canonical_level + torch.log2(box_sizes / canonical_box_size + 1e-8))\n","    # clamp level to (min, max), in case the box size is too large or too small\n","    # for the available feature maps\n","    level_assignments = torch.clamp(level_assignments, min=min_level, max=max_level)\n","    return level_assignments.to(torch.int64) - min_level\n","\n","\n","# Helper Classes\n","class _NewEmptyTensorOp(torch.autograd.Function):\n","    @staticmethod\n","    def forward(ctx, x, new_shape):\n","        ctx.shape = x.shape\n","        return x.new_empty(new_shape)\n","\n","    @staticmethod\n","    def backward(ctx, grad):\n","        shape = ctx.shape\n","        return _NewEmptyTensorOp.apply(grad, shape), None\n","\n","\n","class ShapeSpec(namedtuple(\"_ShapeSpec\", [\"channels\", \"height\", \"width\", \"stride\"])):\n","    def __new__(cls, *, channels=None, height=None, width=None, stride=None):\n","        return super().__new__(cls, channels, height, width, stride)\n","\n","\n","class Box2BoxTransform(object):\n","    \"\"\"\n","    This R-CNN transformation scales the box's width and height\n","    by exp(dw), exp(dh) and shifts a box's center by the offset\n","    (dx * width, dy * height).\n","    \"\"\"\n","\n","    def __init__(self, weights: Tuple[float, float, float, float], scale_clamp: float = None):\n","        \"\"\"\n","        Args:\n","            weights (4-element tuple): Scaling factors that are applied to the\n","                (dx, dy, dw, dh) deltas. In Fast R-CNN, these were originally set\n","                such that the deltas have unit variance; now they are treated as\n","                hyperparameters of the system.\n","            scale_clamp (float): When predicting deltas, the predicted box scaling\n","                factors (dw and dh) are clamped such that they are <= scale_clamp.\n","        \"\"\"\n","        self.weights = weights\n","        if scale_clamp is not None:\n","            self.scale_clamp = scale_clamp\n","        else:\n","            \"\"\"\n","            Value for clamping large dw and dh predictions.\n","            The heuristic is that we clamp such that dw and dh are no larger\n","            than what would transform a 16px box into a 1000px box\n","            (based on a small anchor, 16px, and a typical image size, 1000px).\n","            \"\"\"\n","            self.scale_clamp = math.log(1000.0 / 16)\n","\n","    def get_deltas(self, src_boxes, target_boxes):\n","        \"\"\"\n","        Get box regression transformation deltas (dx, dy, dw, dh) that can be used\n","        to transform the `src_boxes` into the `target_boxes`. That is, the relation\n","        ``target_boxes == self.apply_deltas(deltas, src_boxes)`` is true (unless\n","        any delta is too large and is clamped).\n","        Args:\n","            src_boxes (Tensor): source boxes, e.g., object proposals\n","            target_boxes (Tensor): target of the transformation, e.g., ground-truth\n","                boxes.\n","        \"\"\"\n","        assert isinstance(src_boxes, torch.Tensor), type(src_boxes)\n","        assert isinstance(target_boxes, torch.Tensor), type(target_boxes)\n","\n","        src_widths = src_boxes[:, 2] - src_boxes[:, 0]\n","        src_heights = src_boxes[:, 3] - src_boxes[:, 1]\n","        src_ctr_x = src_boxes[:, 0] + 0.5 * src_widths\n","        src_ctr_y = src_boxes[:, 1] + 0.5 * src_heights\n","\n","        target_widths = target_boxes[:, 2] - target_boxes[:, 0]\n","        target_heights = target_boxes[:, 3] - target_boxes[:, 1]\n","        target_ctr_x = target_boxes[:, 0] + 0.5 * target_widths\n","        target_ctr_y = target_boxes[:, 1] + 0.5 * target_heights\n","\n","        wx, wy, ww, wh = self.weights\n","        dx = wx * (target_ctr_x - src_ctr_x) / src_widths\n","        dy = wy * (target_ctr_y - src_ctr_y) / src_heights\n","        dw = ww * torch.log(target_widths / src_widths)\n","        dh = wh * torch.log(target_heights / src_heights)\n","\n","        deltas = torch.stack((dx, dy, dw, dh), dim=1)\n","        assert (src_widths > 0).all().item(), \"Input boxes to Box2BoxTransform are not valid!\"\n","        return deltas\n","\n","    def apply_deltas(self, deltas, boxes):\n","        \"\"\"\n","        Apply transformation `deltas` (dx, dy, dw, dh) to `boxes`.\n","        Args:\n","            deltas (Tensor): transformation deltas of shape (N, k*4), where k >= 1.\n","                deltas[i] represents k potentially different class-specific\n","                box transformations for the single box boxes[i].\n","            boxes (Tensor): boxes to transform, of shape (N, 4)\n","        \"\"\"\n","        boxes = boxes.to(deltas.dtype)\n","\n","        widths = boxes[:, 2] - boxes[:, 0]\n","        heights = boxes[:, 3] - boxes[:, 1]\n","        ctr_x = boxes[:, 0] + 0.5 * widths\n","        ctr_y = boxes[:, 1] + 0.5 * heights\n","\n","        wx, wy, ww, wh = self.weights\n","        dx = deltas[:, 0::4] / wx\n","        dy = deltas[:, 1::4] / wy\n","        dw = deltas[:, 2::4] / ww\n","        dh = deltas[:, 3::4] / wh\n","\n","        # Prevent sending too large values into torch.exp()\n","        dw = torch.clamp(dw, max=self.scale_clamp)\n","        dh = torch.clamp(dh, max=self.scale_clamp)\n","\n","        pred_ctr_x = dx * widths[:, None] + ctr_x[:, None]\n","        pred_ctr_y = dy * heights[:, None] + ctr_y[:, None]\n","        pred_w = torch.exp(dw) * widths[:, None]\n","        pred_h = torch.exp(dh) * heights[:, None]\n","\n","        pred_boxes = torch.zeros_like(deltas)\n","        pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w  # x1\n","        pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h  # y1\n","        pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w  # x2\n","        pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h  # y2\n","        return pred_boxes\n","\n","\n","class Matcher(object):\n","    \"\"\"\n","    This class assigns to each predicted \"element\" (e.g., a box) a ground-truth\n","    element. Each predicted element will have exactly zero or one matches; each\n","    ground-truth element may be matched to zero or more predicted elements.\n","    The matching is determined by the MxN match_quality_matrix, that characterizes\n","    how well each (ground-truth, prediction)-pair match each other. For example,\n","    if the elements are boxes, this matrix may contain box intersection-over-union\n","    overlap values.\n","    The matcher returns (a) a vector of length N containing the index of the\n","    ground-truth element m in [0, M) that matches to prediction n in [0, N).\n","    (b) a vector of length N containing the labels for each prediction.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        thresholds: List[float],\n","        labels: List[int],\n","        allow_low_quality_matches: bool = False,\n","    ):\n","        \"\"\"\n","        Args:\n","            thresholds (list): a list of thresholds used to stratify predictions\n","                into levels.\n","            labels (list): a list of values to label predictions belonging at\n","                each level. A label can be one of {-1, 0, 1} signifying\n","                {ignore, negative class, positive class}, respectively.\n","            allow_low_quality_matches (bool): if True, produce additional matches or predictions with maximum match quality lower than high_threshold.\n","                For example, thresholds = [0.3, 0.5] labels = [0, -1, 1] All predictions with iou < 0.3 will be marked with 0 and\n","                thus will be considered as false positives while training. All predictions with 0.3 <= iou < 0.5 will be marked with -1 and\n","                thus will be ignored. All predictions with 0.5 <= iou will be marked with 1 and thus will be considered as true positives.\n","        \"\"\"\n","        thresholds = thresholds[:]\n","        assert thresholds[0] > 0\n","        thresholds.insert(0, -float(\"inf\"))\n","        thresholds.append(float(\"inf\"))\n","        assert all([low <= high for (low, high) in zip(thresholds[:-1], thresholds[1:])])\n","        assert all([label_i in [-1, 0, 1] for label_i in labels])\n","        assert len(labels) == len(thresholds) - 1\n","        self.thresholds = thresholds\n","        self.labels = labels\n","        self.allow_low_quality_matches = allow_low_quality_matches\n","\n","    def __call__(self, match_quality_matrix):\n","        \"\"\"\n","        Args:\n","            match_quality_matrix (Tensor[float]): an MxN tensor, containing the pairwise quality between M ground-truth elements and N predicted\n","                elements. All elements must be >= 0 (due to the us of `torch.nonzero` for selecting indices in :meth:`set_low_quality_matches_`).\n","        Returns:\n","            matches (Tensor[int64]): a vector of length N, where matches[i] is a matched ground-truth index in [0, M)\n","            match_labels (Tensor[int8]): a vector of length N, where pred_labels[i] indicates true or false positive or ignored\n","        \"\"\"\n","        assert match_quality_matrix.dim() == 2\n","        if match_quality_matrix.numel() == 0:\n","            default_matches = match_quality_matrix.new_full((match_quality_matrix.size(1),), 0, dtype=torch.int64)\n","            # When no gt boxes exist, we define IOU = 0 and therefore set labels\n","            # to `self.labels[0]`, which usually defaults to background class 0\n","            # To choose to ignore instead,\n","            # can make labels=[-1,0,-1,1] + set appropriate thresholds\n","            default_match_labels = match_quality_matrix.new_full(\n","                (match_quality_matrix.size(1),), self.labels[0], dtype=torch.int8\n","            )\n","            return default_matches, default_match_labels\n","\n","        assert torch.all(match_quality_matrix >= 0)\n","\n","        # match_quality_matrix is M (gt) x N (predicted)\n","        # Max over gt elements (dim 0) to find best gt candidate for each prediction\n","        matched_vals, matches = match_quality_matrix.max(dim=0)\n","\n","        match_labels = matches.new_full(matches.size(), 1, dtype=torch.int8)\n","\n","        for (l, low, high) in zip(self.labels, self.thresholds[:-1], self.thresholds[1:]):\n","            low_high = (matched_vals >= low) & (matched_vals < high)\n","            match_labels[low_high] = l\n","\n","        if self.allow_low_quality_matches:\n","            self.set_low_quality_matches_(match_labels, match_quality_matrix)\n","\n","        return matches, match_labels\n","\n","    def set_low_quality_matches_(self, match_labels, match_quality_matrix):\n","        \"\"\"\n","        Produce additional matches for predictions that have only low-quality matches.\n","        Specifically, for each ground-truth G find the set of predictions that have\n","        maximum overlap with it (including ties); for each prediction in that set, if\n","        it is unmatched, then match it to the ground-truth G.\n","        This function implements the RPN assignment case (i)\n","        in Sec. 3.1.2 of Faster R-CNN.\n","        \"\"\"\n","        # For each gt, find the prediction with which it has highest quality\n","        highest_quality_foreach_gt, _ = match_quality_matrix.max(dim=1)\n","        # Find the highest quality match available, even if it is low, including ties.\n","        # Note that the matches qualities must be positive due to the use of\n","        # `torch.nonzero`.\n","        of_quality_inds = match_quality_matrix == highest_quality_foreach_gt[:, None]\n","        if of_quality_inds.dim() == 0:\n","            (_, pred_inds_with_highest_quality) = of_quality_inds.unsqueeze(0).nonzero().unbind(1)\n","        else:\n","            (_, pred_inds_with_highest_quality) = of_quality_inds.nonzero().unbind(1)\n","        match_labels[pred_inds_with_highest_quality] = 1\n","\n","\n","class RPNOutputs(object):\n","    def __init__(\n","        self,\n","        box2box_transform,\n","        anchor_matcher,\n","        batch_size_per_image,\n","        positive_fraction,\n","        images,\n","        pred_objectness_logits,\n","        pred_anchor_deltas,\n","        anchors,\n","        boundary_threshold=0,\n","        gt_boxes=None,\n","        smooth_l1_beta=0.0,\n","    ):\n","        \"\"\"\n","        Args:\n","            box2box_transform (Box2BoxTransform): :class:`Box2BoxTransform` instance for anchor-proposal transformations.\n","            anchor_matcher (Matcher): :class:`Matcher` instance for matching anchors to ground-truth boxes; used to determine training labels.\n","            batch_size_per_image (int): number of proposals to sample when training\n","            positive_fraction (float): target fraction of sampled proposals that should be positive\n","            images (ImageList): :class:`ImageList` instance representing N input images\n","            pred_objectness_logits (list[Tensor]): A list of L elements. Element i is a tensor of shape (N, A, Hi, W)\n","            pred_anchor_deltas (list[Tensor]): A list of L elements. Element i is a tensor of shape (N, A*4, Hi, Wi)\n","            anchors (list[torch.Tensor]): nested list of boxes. anchors[i][j] at (n, l) stores anchor array for feature map l\n","            boundary_threshold (int): if >= 0, then anchors that extend beyond the image boundary by more than boundary_thresh are not used in training.\n","            gt_boxes (list[Boxes], optional): A list of N elements.\n","            smooth_l1_beta (float): The transition point between L1 and L2 lossn. When set to 0, the loss becomes L1. When +inf, it is ignored\n","        \"\"\"\n","        self.box2box_transform = box2box_transform\n","        self.anchor_matcher = anchor_matcher\n","        self.batch_size_per_image = batch_size_per_image\n","        self.positive_fraction = positive_fraction\n","        self.pred_objectness_logits = pred_objectness_logits\n","        self.pred_anchor_deltas = pred_anchor_deltas\n","\n","        self.anchors = anchors\n","        self.gt_boxes = gt_boxes\n","        self.num_feature_maps = len(pred_objectness_logits)\n","        self.num_images = len(images)\n","        self.boundary_threshold = boundary_threshold\n","        self.smooth_l1_beta = smooth_l1_beta\n","\n","    def _get_ground_truth(self):\n","        raise NotImplementedError()\n","\n","    def predict_proposals(self):\n","        # pred_anchor_deltas: (L, N, ? Hi, Wi)\n","        # anchors:(N, L, -1, B)\n","        # here we loop over specific feature map, NOT images\n","        proposals = []\n","        anchors = self.anchors.transpose(0, 1)\n","        for anchors_i, pred_anchor_deltas_i in zip(anchors, self.pred_anchor_deltas):\n","            B = anchors_i.size(-1)\n","            N, _, Hi, Wi = pred_anchor_deltas_i.shape\n","            anchors_i = anchors_i.flatten(start_dim=0, end_dim=1)\n","            pred_anchor_deltas_i = pred_anchor_deltas_i.view(N, -1, B, Hi, Wi).permute(0, 3, 4, 1, 2).reshape(-1, B)\n","            proposals_i = self.box2box_transform.apply_deltas(pred_anchor_deltas_i, anchors_i)\n","            # Append feature map proposals with shape (N, Hi*Wi*A, B)\n","            proposals.append(proposals_i.view(N, -1, B))\n","        proposals = torch.stack(proposals)\n","        return proposals\n","\n","    def predict_objectness_logits(self):\n","        \"\"\"\n","        Returns:\n","            pred_objectness_logits (list[Tensor]) -> (N, Hi*Wi*A).\n","        \"\"\"\n","        pred_objectness_logits = [\n","            # Reshape: (N, A, Hi, Wi) -> (N, Hi, Wi, A) -> (N, Hi*Wi*A)\n","            score.permute(0, 2, 3, 1).reshape(self.num_images, -1)\n","            for score in self.pred_objectness_logits\n","        ]\n","        return pred_objectness_logits\n","\n","\n","# Main Classes\n","class Conv2d(nn.Conv2d):\n","    def __init__(self, *args, **kwargs):\n","        norm = kwargs.pop(\"norm\", None)\n","        activation = kwargs.pop(\"activation\", None)\n","        super().__init__(*args, **kwargs)\n","\n","        self.norm = norm\n","        self.activation = activation\n","\n","    def forward(self, x):\n","        if x.numel() == 0 and self.training:\n","            assert not isinstance(self.norm, nn.SyncBatchNorm)\n","        if x.numel() == 0:\n","            assert not isinstance(self.norm, nn.GroupNorm)\n","            output_shape = [\n","                (i + 2 * p - (di * (k - 1) + 1)) // s + 1\n","                for i, p, di, k, s in zip(\n","                    x.shape[-2:],\n","                    self.padding,\n","                    self.dilation,\n","                    self.kernel_size,\n","                    self.stride,\n","                )\n","            ]\n","            output_shape = [x.shape[0], self.weight.shape[0]] + output_shape\n","            empty = _NewEmptyTensorOp.apply(x, output_shape)\n","            if self.training:\n","                _dummy = sum(x.view(-1)[0] for x in self.parameters()) * 0.0\n","                return empty + _dummy\n","            else:\n","                return empty\n","\n","        x = super().forward(x)\n","        if self.norm is not None:\n","            x = self.norm(x)\n","        if self.activation is not None:\n","            x = self.activation(x)\n","        return x\n","\n","\n","class LastLevelMaxPool(nn.Module):\n","    \"\"\"\n","    This module is used in the original FPN to generate a downsampled P6 feature from P5.\n","    \"\"\"\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.num_levels = 1\n","        self.in_feature = \"p5\"\n","\n","    def forward(self, x):\n","        return [nn.functional.max_pool2d(x, kernel_size=1, stride=2, padding=0)]\n","\n","\n","class LastLevelP6P7(nn.Module):\n","    \"\"\"\n","    This module is used in RetinaNet to generate extra layers, P6 and P7 from C5 feature.\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.num_levels = 2\n","        self.in_feature = \"res5\"\n","        self.p6 = nn.Conv2d(in_channels, out_channels, 3, 2, 1)\n","        self.p7 = nn.Conv2d(out_channels, out_channels, 3, 2, 1)\n","\n","    def forward(self, c5):\n","        p6 = self.p6(c5)\n","        p7 = self.p7(nn.functional.relu(p6))\n","        return [p6, p7]\n","\n","\n","class BasicStem(nn.Module):\n","    def __init__(self, in_channels=3, out_channels=64, norm=\"BN\", caffe_maxpool=False):\n","        super().__init__()\n","        self.conv1 = Conv2d(\n","            in_channels,\n","            out_channels,\n","            kernel_size=7,\n","            stride=2,\n","            padding=3,\n","            bias=False,\n","            norm=get_norm(norm, out_channels),\n","        )\n","        self.caffe_maxpool = caffe_maxpool\n","        # use pad 1 instead of pad zero\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = nn.functional.relu_(x)\n","        if self.caffe_maxpool:\n","            x = nn.functional.max_pool2d(x, kernel_size=3, stride=2, padding=0, ceil_mode=True)\n","        else:\n","            x = nn.functional.max_pool2d(x, kernel_size=3, stride=2, padding=1)\n","        return x\n","\n","    @property\n","    def out_channels(self):\n","        return self.conv1.out_channels\n","\n","    @property\n","    def stride(self):\n","        return 4  # = stride 2 conv -> stride 2 max pool\n","\n","\n","class ResNetBlockBase(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.stride = stride\n","\n","    def freeze(self):\n","        for p in self.parameters():\n","            p.requires_grad = False\n","        return self\n","\n","\n","class BottleneckBlock(ResNetBlockBase):\n","    def __init__(\n","        self,\n","        in_channels,\n","        out_channels,\n","        bottleneck_channels,\n","        stride=1,\n","        num_groups=1,\n","        norm=\"BN\",\n","        stride_in_1x1=False,\n","        dilation=1,\n","    ):\n","        super().__init__(in_channels, out_channels, stride)\n","\n","        if in_channels != out_channels:\n","            self.shortcut = Conv2d(\n","                in_channels,\n","                out_channels,\n","                kernel_size=1,\n","                stride=stride,\n","                bias=False,\n","                norm=get_norm(norm, out_channels),\n","            )\n","        else:\n","            self.shortcut = None\n","\n","        # The original MSRA ResNet models have stride in the first 1x1 conv\n","        # The subsequent fb.torch.resnet and Caffe2 ResNe[X]t implementations have\n","        # stride in the 3x3 conv\n","        stride_1x1, stride_3x3 = (stride, 1) if stride_in_1x1 else (1, stride)\n","\n","        self.conv1 = Conv2d(\n","            in_channels,\n","            bottleneck_channels,\n","            kernel_size=1,\n","            stride=stride_1x1,\n","            bias=False,\n","            norm=get_norm(norm, bottleneck_channels),\n","        )\n","\n","        self.conv2 = Conv2d(\n","            bottleneck_channels,\n","            bottleneck_channels,\n","            kernel_size=3,\n","            stride=stride_3x3,\n","            padding=1 * dilation,\n","            bias=False,\n","            groups=num_groups,\n","            dilation=dilation,\n","            norm=get_norm(norm, bottleneck_channels),\n","        )\n","\n","        self.conv3 = Conv2d(\n","            bottleneck_channels,\n","            out_channels,\n","            kernel_size=1,\n","            bias=False,\n","            norm=get_norm(norm, out_channels),\n","        )\n","\n","    def forward(self, x):\n","        out = self.conv1(x)\n","        out = nn.functional.relu_(out)\n","\n","        out = self.conv2(out)\n","        out = nn.functional.relu_(out)\n","\n","        out = self.conv3(out)\n","\n","        if self.shortcut is not None:\n","            shortcut = self.shortcut(x)\n","        else:\n","            shortcut = x\n","\n","        out += shortcut\n","        out = nn.functional.relu_(out)\n","        return out\n","\n","\n","class Backbone(nn.Module, metaclass=ABCMeta):\n","    def __init__(self):\n","        super().__init__()\n","\n","    @abstractmethod\n","    def forward(self):\n","        pass\n","\n","    @property\n","    def size_divisibility(self):\n","        \"\"\"\n","        Some backbones require the input height and width to be divisible by a specific integer. This is\n","        typically true for encoder / decoder type networks with lateral connection (e.g., FPN) for which feature maps need to match\n","        dimension in the \"bottom up\" and \"top down\" paths. Set to 0 if no specific input size divisibility is required.\n","        \"\"\"\n","        return 0\n","\n","    def output_shape(self):\n","        return {\n","            name: ShapeSpec(\n","                channels=self._out_feature_channels[name],\n","                stride=self._out_feature_strides[name],\n","            )\n","            for name in self._out_features\n","        }\n","\n","    @property\n","    def out_features(self):\n","        \"\"\"deprecated\"\"\"\n","        return self._out_features\n","\n","    @property\n","    def out_feature_strides(self):\n","        \"\"\"deprecated\"\"\"\n","        return {f: self._out_feature_strides[f] for f in self._out_features}\n","\n","    @property\n","    def out_feature_channels(self):\n","        \"\"\"deprecated\"\"\"\n","        return {f: self._out_feature_channels[f] for f in self._out_features}\n","\n","\n","class ResNet(Backbone):\n","    def __init__(self, stem, stages, num_classes=None, out_features=None):\n","        \"\"\"\n","        Args:\n","            stem (nn.Module): a stem module\n","            stages (list[list[ResNetBlock]]): several (typically 4) stages, each contains multiple :class:`ResNetBlockBase`.\n","            num_classes (None or int): if None, will not perform classification.\n","            out_features (list[str]): name of the layers whose outputs should be returned in forward. Can be anything in:\n","            \"stem\", \"linear\", or \"res2\" ... If None, will return the output of the last layer.\n","        \"\"\"\n","        super(ResNet, self).__init__()\n","        self.stem = stem\n","        self.num_classes = num_classes\n","\n","        current_stride = self.stem.stride\n","        self._out_feature_strides = {\"stem\": current_stride}\n","        self._out_feature_channels = {\"stem\": self.stem.out_channels}\n","\n","        self.stages_and_names = []\n","        for i, blocks in enumerate(stages):\n","            for block in blocks:\n","                assert isinstance(block, ResNetBlockBase), block\n","                curr_channels = block.out_channels\n","            stage = nn.Sequential(*blocks)\n","            name = \"res\" + str(i + 2)\n","            self.add_module(name, stage)\n","            self.stages_and_names.append((stage, name))\n","            self._out_feature_strides[name] = current_stride = int(\n","                current_stride * np.prod([k.stride for k in blocks])\n","            )\n","            self._out_feature_channels[name] = blocks[-1].out_channels\n","\n","        if num_classes is not None:\n","            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n","            self.linear = nn.Linear(curr_channels, num_classes)\n","\n","            # Sec 5.1 in \"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour\":\n","            # \"The 1000-way fully-connected layer is initialized by\n","            # drawing weights from a zero-mean Gaussian with std of 0.01.\"\n","            nn.init.normal_(self.linear.weight, stddev=0.01)\n","            name = \"linear\"\n","\n","        if out_features is None:\n","            out_features = [name]\n","        self._out_features = out_features\n","        assert len(self._out_features)\n","        children = [x[0] for x in self.named_children()]\n","        for out_feature in self._out_features:\n","            assert out_feature in children, \"Available children: {}\".format(\", \".join(children))\n","\n","    def forward(self, x):\n","        outputs = {}\n","        x = self.stem(x)\n","        if \"stem\" in self._out_features:\n","            outputs[\"stem\"] = x\n","        for stage, name in self.stages_and_names:\n","            x = stage(x)\n","            if name in self._out_features:\n","                outputs[name] = x\n","        if self.num_classes is not None:\n","            x = self.avgpool(x)\n","            x = self.linear(x)\n","            if \"linear\" in self._out_features:\n","                outputs[\"linear\"] = x\n","        return outputs\n","\n","    def output_shape(self):\n","        return {\n","            name: ShapeSpec(\n","                channels=self._out_feature_channels[name],\n","                stride=self._out_feature_strides[name],\n","            )\n","            for name in self._out_features\n","        }\n","\n","    @staticmethod\n","    def make_stage(\n","        block_class,\n","        num_blocks,\n","        first_stride=None,\n","        *,\n","        in_channels,\n","        out_channels,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        Usually, layers that produce the same feature map spatial size\n","        are defined as one \"stage\".\n","        Under such definition, stride_per_block[1:] should all be 1.\n","        \"\"\"\n","        if first_stride is not None:\n","            assert \"stride\" not in kwargs and \"stride_per_block\" not in kwargs\n","            kwargs[\"stride_per_block\"] = [first_stride] + [1] * (num_blocks - 1)\n","        blocks = []\n","        for i in range(num_blocks):\n","            curr_kwargs = {}\n","            for k, v in kwargs.items():\n","                if k.endswith(\"_per_block\"):\n","                    assert len(v) == num_blocks, (\n","                        f\"Argument '{k}' of make_stage should have the \" f\"same length as num_blocks={num_blocks}.\"\n","                    )\n","                    newk = k[: -len(\"_per_block\")]\n","                    assert newk not in kwargs, f\"Cannot call make_stage with both {k} and {newk}!\"\n","                    curr_kwargs[newk] = v[i]\n","                else:\n","                    curr_kwargs[k] = v\n","\n","            blocks.append(block_class(in_channels=in_channels, out_channels=out_channels, **curr_kwargs))\n","            in_channels = out_channels\n","\n","        return blocks\n","\n","\n","class ROIPooler(nn.Module):\n","    \"\"\"\n","    Region of interest feature map pooler that supports pooling from one or more\n","    feature maps.\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        output_size,\n","        scales,\n","        sampling_ratio,\n","        canonical_box_size=224,\n","        canonical_level=4,\n","    ):\n","        super().__init__()\n","        # assumption that stride is a power of 2.\n","        min_level = -math.log2(scales[0])\n","        max_level = -math.log2(scales[-1])\n","\n","        # a bunch of testing\n","        assert math.isclose(min_level, int(min_level)) and math.isclose(max_level, int(max_level))\n","        assert len(scales) == max_level - min_level + 1, \"not pyramid\"\n","        assert 0 < min_level and min_level <= max_level\n","        if isinstance(output_size, int):\n","            output_size = (output_size, output_size)\n","        assert len(output_size) == 2 and isinstance(output_size[0], int) and isinstance(output_size[1], int)\n","        if len(scales) > 1:\n","            assert min_level <= canonical_level and canonical_level <= max_level\n","        assert canonical_box_size > 0\n","\n","        self.output_size = output_size\n","        self.min_level = int(min_level)\n","        self.max_level = int(max_level)\n","        self.level_poolers = nn.ModuleList(RoIPool(output_size, spatial_scale=scale) for scale in scales)\n","        self.canonical_level = canonical_level\n","        self.canonical_box_size = canonical_box_size\n","\n","    def forward(self, feature_maps, boxes):\n","        \"\"\"\n","        Args:\n","            feature_maps: List[torch.Tensor(N,C,W,H)]\n","            box_lists: list[torch.Tensor])\n","        Returns:\n","            A tensor of shape(N*B, Channels, output_size, output_size)\n","        \"\"\"\n","        x = [v for v in feature_maps.values()]\n","        num_level_assignments = len(self.level_poolers)\n","        assert len(x) == num_level_assignments and len(boxes) == x[0].size(0)\n","\n","        pooler_fmt_boxes = convert_boxes_to_pooler_format(boxes)\n","\n","        if num_level_assignments == 1:\n","            return self.level_poolers[0](x[0], pooler_fmt_boxes)\n","\n","        level_assignments = assign_boxes_to_levels(\n","            boxes,\n","            self.min_level,\n","            self.max_level,\n","            self.canonical_box_size,\n","            self.canonical_level,\n","        )\n","\n","        num_boxes = len(pooler_fmt_boxes)\n","        num_channels = x[0].shape[1]\n","        output_size = self.output_size[0]\n","\n","        dtype, device = x[0].dtype, x[0].device\n","        output = torch.zeros(\n","            (num_boxes, num_channels, output_size, output_size),\n","            dtype=dtype,\n","            device=device,\n","        )\n","\n","        for level, (x_level, pooler) in enumerate(zip(x, self.level_poolers)):\n","            inds = torch.nonzero(level_assignments == level).squeeze(1)\n","            pooler_fmt_boxes_level = pooler_fmt_boxes[inds]\n","            output[inds] = pooler(x_level, pooler_fmt_boxes_level)\n","\n","        return output\n","\n","\n","class ROIOutputs(object):\n","    def __init__(self, cfg, training=False):\n","        self.smooth_l1_beta = cfg.ROI_BOX_HEAD.SMOOTH_L1_BETA\n","        self.box2box_transform = Box2BoxTransform(weights=cfg.ROI_BOX_HEAD.BBOX_REG_WEIGHTS)\n","        self.training = training\n","        self.score_thresh = cfg.ROI_HEADS.SCORE_THRESH_TEST\n","        self.min_detections = cfg.MIN_DETECTIONS\n","        self.max_detections = cfg.MAX_DETECTIONS\n","\n","        nms_thresh = cfg.ROI_HEADS.NMS_THRESH_TEST\n","        if not isinstance(nms_thresh, list):\n","            nms_thresh = [nms_thresh]\n","        self.nms_thresh = nms_thresh\n","\n","    def _predict_boxes(self, proposals, box_deltas, preds_per_image):\n","        num_pred = box_deltas.size(0)\n","        B = proposals[0].size(-1)\n","        K = box_deltas.size(-1) // B\n","        box_deltas = box_deltas.view(num_pred * K, B)\n","        proposals = torch.cat(proposals, dim=0).unsqueeze(-2).expand(num_pred, K, B)\n","        proposals = proposals.reshape(-1, B)\n","        boxes = self.box2box_transform.apply_deltas(box_deltas, proposals)\n","        return boxes.view(num_pred, K * B).split(preds_per_image, dim=0)\n","\n","    def _predict_objs(self, obj_logits, preds_per_image):\n","        probs = nn.functional.softmax(obj_logits, dim=-1)\n","        probs = probs.split(preds_per_image, dim=0)\n","        return probs\n","\n","    def _predict_attrs(self, attr_logits, preds_per_image):\n","        attr_logits = attr_logits[..., :-1].softmax(-1)\n","        attr_probs, attrs = attr_logits.max(-1)\n","        return attr_probs.split(preds_per_image, dim=0), attrs.split(preds_per_image, dim=0)\n","\n","    @torch.no_grad()\n","    def inference(\n","        self,\n","        obj_logits,\n","        attr_logits,\n","        box_deltas,\n","        pred_boxes,\n","        features,\n","        sizes,\n","        scales=None,\n","    ):\n","        # only the pred boxes is the\n","        preds_per_image = [p.size(0) for p in pred_boxes]\n","        boxes_all = self._predict_boxes(pred_boxes, box_deltas, preds_per_image)\n","        obj_scores_all = self._predict_objs(obj_logits, preds_per_image)  # list of length N\n","        attr_probs_all, attrs_all = self._predict_attrs(attr_logits, preds_per_image)\n","        features = features.split(preds_per_image, dim=0)\n","\n","        # fun for each image too, also I can experiment and do multiple images\n","        final_results = []\n","        zipped = zip(boxes_all, obj_scores_all, attr_probs_all, attrs_all, sizes)\n","        for i, (boxes, obj_scores, attr_probs, attrs, size) in enumerate(zipped):\n","            for nms_t in self.nms_thresh:\n","                outputs = do_nms(\n","                    boxes,\n","                    obj_scores,\n","                    size,\n","                    self.score_thresh,\n","                    nms_t,\n","                    self.min_detections,\n","                    self.max_detections,\n","                )\n","                if outputs is not None:\n","                    max_boxes, max_scores, classes, ids = outputs\n","                    break\n","\n","            if scales is not None:\n","                scale_yx = scales[i]\n","                max_boxes[:, 0::2] *= scale_yx[1]\n","                max_boxes[:, 1::2] *= scale_yx[0]\n","\n","            final_results.append(\n","                (\n","                    max_boxes,\n","                    classes,\n","                    max_scores,\n","                    attrs[ids],\n","                    attr_probs[ids],\n","                    features[i][ids],\n","                )\n","            )\n","        boxes, classes, class_probs, attrs, attr_probs, roi_features = map(list, zip(*final_results))\n","        return boxes, classes, class_probs, attrs, attr_probs, roi_features\n","\n","    def training(self, obj_logits, attr_logits, box_deltas, pred_boxes, features, sizes):\n","        pass\n","\n","    def __call__(\n","        self,\n","        obj_logits,\n","        attr_logits,\n","        box_deltas,\n","        pred_boxes,\n","        features,\n","        sizes,\n","        scales=None,\n","    ):\n","        if self.training:\n","            raise NotImplementedError()\n","        return self.inference(\n","            obj_logits,\n","            attr_logits,\n","            box_deltas,\n","            pred_boxes,\n","            features,\n","            sizes,\n","            scales=scales,\n","        )\n","\n","\n","class Res5ROIHeads(nn.Module):\n","    \"\"\"\n","    ROIHeads perform all per-region computation in an R-CNN.\n","    It contains logic of cropping the regions, extract per-region features\n","    (by the res-5 block in this case), and make per-region predictions.\n","    \"\"\"\n","\n","    def __init__(self, cfg, input_shape):\n","        super().__init__()\n","        self.batch_size_per_image = cfg.RPN.BATCH_SIZE_PER_IMAGE\n","        self.positive_sample_fraction = cfg.ROI_HEADS.POSITIVE_FRACTION\n","        self.in_features = cfg.ROI_HEADS.IN_FEATURES\n","        self.num_classes = cfg.ROI_HEADS.NUM_CLASSES\n","        self.proposal_append_gt = cfg.ROI_HEADS.PROPOSAL_APPEND_GT\n","        self.feature_strides = {k: v.stride for k, v in input_shape.items()}\n","        self.feature_channels = {k: v.channels for k, v in input_shape.items()}\n","        self.cls_agnostic_bbox_reg = cfg.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG\n","        self.stage_channel_factor = 2 ** 3  # res5 is 8x res2\n","        self.out_channels = cfg.RESNETS.RES2_OUT_CHANNELS * self.stage_channel_factor\n","\n","        # self.proposal_matcher = Matcher(\n","        #     cfg.ROI_HEADS.IOU_THRESHOLDS,\n","        #     cfg.ROI_HEADS.IOU_LABELS,\n","        #     allow_low_quality_matches=False,\n","        # )\n","\n","        pooler_resolution = cfg.ROI_BOX_HEAD.POOLER_RESOLUTION\n","        pooler_scales = (1.0 / self.feature_strides[self.in_features[0]],)\n","        sampling_ratio = cfg.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO\n","        res5_halve = cfg.ROI_BOX_HEAD.RES5HALVE\n","        use_attr = cfg.ROI_BOX_HEAD.ATTR\n","        num_attrs = cfg.ROI_BOX_HEAD.NUM_ATTRS\n","\n","        self.pooler = ROIPooler(\n","            output_size=pooler_resolution,\n","            scales=pooler_scales,\n","            sampling_ratio=sampling_ratio,\n","        )\n","\n","        self.res5 = self._build_res5_block(cfg)\n","        if not res5_halve:\n","            \"\"\"\n","            Modifications for VG in RoI heads:\n","            1. Change the stride of conv1 and shortcut in Res5.Block1 from 2 to 1\n","            2. Modifying all conv2 with (padding: 1 --> 2) and (dilation: 1 --> 2)\n","            \"\"\"\n","            self.res5[0].conv1.stride = (1, 1)\n","            self.res5[0].shortcut.stride = (1, 1)\n","            for i in range(3):\n","                self.res5[i].conv2.padding = (2, 2)\n","                self.res5[i].conv2.dilation = (2, 2)\n","\n","        self.box_predictor = FastRCNNOutputLayers(\n","            self.out_channels,\n","            self.num_classes,\n","            self.cls_agnostic_bbox_reg,\n","            use_attr=use_attr,\n","            num_attrs=num_attrs,\n","        )\n","\n","    def _build_res5_block(self, cfg):\n","        stage_channel_factor = self.stage_channel_factor  # res5 is 8x res2\n","        num_groups = cfg.RESNETS.NUM_GROUPS\n","        width_per_group = cfg.RESNETS.WIDTH_PER_GROUP\n","        bottleneck_channels = num_groups * width_per_group * stage_channel_factor\n","        out_channels = self.out_channels\n","        stride_in_1x1 = cfg.RESNETS.STRIDE_IN_1X1\n","        norm = cfg.RESNETS.NORM\n","\n","        blocks = ResNet.make_stage(\n","            BottleneckBlock,\n","            3,\n","            first_stride=2,\n","            in_channels=out_channels // 2,\n","            bottleneck_channels=bottleneck_channels,\n","            out_channels=out_channels,\n","            num_groups=num_groups,\n","            norm=norm,\n","            stride_in_1x1=stride_in_1x1,\n","        )\n","        return nn.Sequential(*blocks)\n","\n","    def _shared_roi_transform(self, features, boxes):\n","        x = self.pooler(features, boxes)\n","        return self.res5(x)\n","\n","    def forward(self, features, proposal_boxes, gt_boxes=None):\n","        if self.training:\n","            \"\"\"\n","            see https://github.com/airsplay/py-bottom-up-attention/\\\n","                    blob/master/detectron2/modeling/roi_heads/roi_heads.py\n","            \"\"\"\n","            raise NotImplementedError()\n","\n","        assert not proposal_boxes[0].requires_grad\n","        box_features = self._shared_roi_transform(features, proposal_boxes)\n","        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1\n","        obj_logits, attr_logits, pred_proposal_deltas = self.box_predictor(feature_pooled)\n","        return obj_logits, attr_logits, pred_proposal_deltas, feature_pooled\n","\n","\n","class AnchorGenerator(nn.Module):\n","    \"\"\"\n","    For a set of image sizes and feature maps, computes a set of anchors.\n","    \"\"\"\n","\n","    def __init__(self, cfg, input_shape: List[ShapeSpec]):\n","        super().__init__()\n","        sizes = cfg.ANCHOR_GENERATOR.SIZES\n","        aspect_ratios = cfg.ANCHOR_GENERATOR.ASPECT_RATIOS\n","        self.strides = [x.stride for x in input_shape]\n","        self.offset = cfg.ANCHOR_GENERATOR.OFFSET\n","        assert 0.0 <= self.offset < 1.0, self.offset\n","\n","        \"\"\"\n","        sizes (list[list[int]]): sizes[i] is the list of anchor sizes for feat map i\n","            1. given in absolute lengths in units of the input image;\n","            2. they do not dynamically scale if the input image size changes.\n","        aspect_ratios (list[list[float]])\n","        strides (list[int]): stride of each input feature.\n","        \"\"\"\n","\n","        self.num_features = len(self.strides)\n","        self.cell_anchors = nn.ParameterList(self._calculate_anchors(sizes, aspect_ratios))\n","        self._spacial_feat_dim = 4\n","\n","    def _calculate_anchors(self, sizes, aspect_ratios):\n","        # If one size (or aspect ratio) is specified and there are multiple feature\n","        # maps, then we \"broadcast\" anchors of that single size (or aspect ratio)\n","        if len(sizes) == 1:\n","            sizes *= self.num_features\n","        if len(aspect_ratios) == 1:\n","            aspect_ratios *= self.num_features\n","        assert self.num_features == len(sizes)\n","        assert self.num_features == len(aspect_ratios)\n","\n","        cell_anchors = [self.generate_cell_anchors(s, a).float() for s, a in zip(sizes, aspect_ratios)]\n","\n","        return cell_anchors\n","\n","    @property\n","    def box_dim(self):\n","        return self._spacial_feat_dim\n","\n","    @property\n","    def num_cell_anchors(self):\n","        \"\"\"\n","        Returns:\n","            list[int]: Each int is the number of anchors at every pixel location, on that feature map.\n","        \"\"\"\n","        return [len(cell_anchors) for cell_anchors in self.cell_anchors]\n","\n","    def grid_anchors(self, grid_sizes):\n","        anchors = []\n","        for (size, stride, base_anchors) in zip(grid_sizes, self.strides, self.cell_anchors):\n","            shift_x, shift_y = _create_grid_offsets(size, stride, self.offset, base_anchors.device)\n","            shifts = torch.stack((shift_x, shift_y, shift_x, shift_y), dim=1)\n","\n","            anchors.append((shifts.view(-1, 1, 4) + base_anchors.view(1, -1, 4)).reshape(-1, 4))\n","\n","        return anchors\n","\n","    def generate_cell_anchors(self, sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.5, 1, 2)):\n","        \"\"\"\n","        anchors are continuous geometric rectangles\n","        centered on one feature map point sample.\n","        We can later build the set of anchors\n","        for the entire feature map by tiling these tensors\n","        \"\"\"\n","\n","        anchors = []\n","        for size in sizes:\n","            area = size ** 2.0\n","            for aspect_ratio in aspect_ratios:\n","                w = math.sqrt(area / aspect_ratio)\n","                h = aspect_ratio * w\n","                x0, y0, x1, y1 = -w / 2.0, -h / 2.0, w / 2.0, h / 2.0\n","                anchors.append([x0, y0, x1, y1])\n","        return nn.Parameter(torch.tensor(anchors))\n","\n","    def forward(self, features):\n","        \"\"\"\n","        Args:\n","            features List[torch.Tensor]: list of feature maps on which to generate anchors.\n","        Returns:\n","            torch.Tensor: a list of #image elements.\n","        \"\"\"\n","        num_images = features[0].size(0)\n","        grid_sizes = [feature_map.shape[-2:] for feature_map in features]\n","        anchors_over_all_feature_maps = self.grid_anchors(grid_sizes)\n","        anchors_over_all_feature_maps = torch.stack(anchors_over_all_feature_maps)\n","        return anchors_over_all_feature_maps.unsqueeze(0).repeat_interleave(num_images, dim=0)\n","\n","\n","class RPNHead(nn.Module):\n","    \"\"\"\n","    RPN classification and regression heads. Uses a 3x3 conv to produce a shared\n","    hidden state from which one 1x1 conv predicts objectness logits for each anchor\n","    and a second 1x1 conv predicts bounding-box deltas specifying how to deform\n","    each anchor into an object proposal.\n","    \"\"\"\n","\n","    def __init__(self, cfg, input_shape: List[ShapeSpec]):\n","        super().__init__()\n","\n","        # Standard RPN is shared across levels:\n","        in_channels = [s.channels for s in input_shape]\n","        assert len(set(in_channels)) == 1, \"Each level must have the same channel!\"\n","        in_channels = in_channels[0]\n","\n","        anchor_generator = AnchorGenerator(cfg, input_shape)\n","        num_cell_anchors = anchor_generator.num_cell_anchors\n","        box_dim = anchor_generator.box_dim\n","        assert len(set(num_cell_anchors)) == 1, \"Each level must have the same number of cell anchors\"\n","        num_cell_anchors = num_cell_anchors[0]\n","\n","        if cfg.PROPOSAL_GENERATOR.HIDDEN_CHANNELS == -1:\n","            hid_channels = in_channels\n","        else:\n","            hid_channels = cfg.PROPOSAL_GENERATOR.HIDDEN_CHANNELS\n","            # Modifications for VG in RPN (modeling/proposal_generator/rpn.py)\n","            # Use hidden dim  instead fo the same dim as Res4 (in_channels)\n","\n","        # 3x3 conv for the hidden representation\n","        self.conv = nn.Conv2d(in_channels, hid_channels, kernel_size=3, stride=1, padding=1)\n","        # 1x1 conv for predicting objectness logits\n","        self.objectness_logits = nn.Conv2d(hid_channels, num_cell_anchors, kernel_size=1, stride=1)\n","        # 1x1 conv for predicting box2box transform deltas\n","        self.anchor_deltas = nn.Conv2d(hid_channels, num_cell_anchors * box_dim, kernel_size=1, stride=1)\n","\n","        for layer in [self.conv, self.objectness_logits, self.anchor_deltas]:\n","            nn.init.normal_(layer.weight, std=0.01)\n","            nn.init.constant_(layer.bias, 0)\n","\n","    def forward(self, features):\n","        \"\"\"\n","        Args:\n","            features (list[Tensor]): list of feature maps\n","        \"\"\"\n","        pred_objectness_logits = []\n","        pred_anchor_deltas = []\n","        for x in features:\n","            t = nn.functional.relu(self.conv(x))\n","            pred_objectness_logits.append(self.objectness_logits(t))\n","            pred_anchor_deltas.append(self.anchor_deltas(t))\n","        return pred_objectness_logits, pred_anchor_deltas\n","\n","\n","class RPN(nn.Module):\n","    \"\"\"\n","    Region Proposal Network, introduced by the Faster R-CNN paper.\n","    \"\"\"\n","\n","    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):\n","        super().__init__()\n","\n","        self.min_box_side_len = cfg.PROPOSAL_GENERATOR.MIN_SIZE\n","        self.in_features = cfg.RPN.IN_FEATURES\n","        self.nms_thresh = cfg.RPN.NMS_THRESH\n","        self.batch_size_per_image = cfg.RPN.BATCH_SIZE_PER_IMAGE\n","        self.positive_fraction = cfg.RPN.POSITIVE_FRACTION\n","        self.smooth_l1_beta = cfg.RPN.SMOOTH_L1_BETA\n","        self.loss_weight = cfg.RPN.LOSS_WEIGHT\n","\n","        self.pre_nms_topk = {\n","            True: cfg.RPN.PRE_NMS_TOPK_TRAIN,\n","            False: cfg.RPN.PRE_NMS_TOPK_TEST,\n","        }\n","        self.post_nms_topk = {\n","            True: cfg.RPN.POST_NMS_TOPK_TRAIN,\n","            False: cfg.RPN.POST_NMS_TOPK_TEST,\n","        }\n","        self.boundary_threshold = cfg.RPN.BOUNDARY_THRESH\n","\n","        self.anchor_generator = AnchorGenerator(cfg, [input_shape[f] for f in self.in_features])\n","        self.box2box_transform = Box2BoxTransform(weights=cfg.RPN.BBOX_REG_WEIGHTS)\n","        self.anchor_matcher = Matcher(\n","            cfg.RPN.IOU_THRESHOLDS,\n","            cfg.RPN.IOU_LABELS,\n","            allow_low_quality_matches=True,\n","        )\n","        self.rpn_head = RPNHead(cfg, [input_shape[f] for f in self.in_features])\n","\n","    def training(self, images, image_shapes, features, gt_boxes):\n","        pass\n","\n","    def inference(self, outputs, images, image_shapes, features, gt_boxes=None):\n","        outputs = find_top_rpn_proposals(\n","            outputs.predict_proposals(),\n","            outputs.predict_objectness_logits(),\n","            images,\n","            image_shapes,\n","            self.nms_thresh,\n","            self.pre_nms_topk[self.training],\n","            self.post_nms_topk[self.training],\n","            self.min_box_side_len,\n","            self.training,\n","        )\n","\n","        results = []\n","        for img in outputs:\n","            im_boxes, img_box_logits = img\n","            img_box_logits, inds = img_box_logits.sort(descending=True)\n","            im_boxes = im_boxes[inds]\n","            results.append((im_boxes, img_box_logits))\n","\n","        (proposal_boxes, logits) = tuple(map(list, zip(*results)))\n","        return proposal_boxes, logits\n","\n","    def forward(self, images, image_shapes, features, gt_boxes=None):\n","        \"\"\"\n","        Args:\n","            images (torch.Tensor): input images of length `N`\n","            features (dict[str: Tensor])\n","            gt_instances\n","        \"\"\"\n","        # features is dict, key = block level, v = feature_map\n","        features = [features[f] for f in self.in_features]\n","        pred_objectness_logits, pred_anchor_deltas = self.rpn_head(features)\n","        anchors = self.anchor_generator(features)\n","        outputs = RPNOutputs(\n","            self.box2box_transform,\n","            self.anchor_matcher,\n","            self.batch_size_per_image,\n","            self.positive_fraction,\n","            images,\n","            pred_objectness_logits,\n","            pred_anchor_deltas,\n","            anchors,\n","            self.boundary_threshold,\n","            gt_boxes,\n","            self.smooth_l1_beta,\n","        )\n","        # For RPN-only models, the proposals are the final output\n","\n","        if self.training:\n","            raise NotImplementedError()\n","            return self.training(outputs, images, image_shapes, features, gt_boxes)\n","        else:\n","            return self.inference(outputs, images, image_shapes, features, gt_boxes)\n","\n","\n","class FastRCNNOutputLayers(nn.Module):\n","    \"\"\"\n","    Two linear layers for predicting Fast R-CNN outputs:\n","      (1) proposal-to-detection box regression deltas\n","      (2) classification scores\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        input_size,\n","        num_classes,\n","        cls_agnostic_bbox_reg,\n","        box_dim=4,\n","        use_attr=False,\n","        num_attrs=-1,\n","    ):\n","        \"\"\"\n","        Args:\n","            input_size (int): channels, or (channels, height, width)\n","            num_classes (int)\n","            cls_agnostic_bbox_reg (bool)\n","            box_dim (int)\n","        \"\"\"\n","        super().__init__()\n","\n","        if not isinstance(input_size, int):\n","            input_size = np.prod(input_size)\n","\n","        # (do + 1 for background class)\n","        self.cls_score = nn.Linear(input_size, num_classes + 1)\n","        num_bbox_reg_classes = 1 if cls_agnostic_bbox_reg else num_classes\n","        self.bbox_pred = nn.Linear(input_size, num_bbox_reg_classes * box_dim)\n","\n","        self.use_attr = use_attr\n","        if use_attr:\n","            \"\"\"\n","            Modifications for VG in RoI heads\n","            Embedding: {num_classes + 1} --> {input_size // 8}\n","            Linear: {input_size + input_size // 8} --> {input_size // 4}\n","            Linear: {input_size // 4} --> {num_attrs + 1}\n","            \"\"\"\n","            self.cls_embedding = nn.Embedding(num_classes + 1, input_size // 8)\n","            self.fc_attr = nn.Linear(input_size + input_size // 8, input_size // 4)\n","            self.attr_score = nn.Linear(input_size // 4, num_attrs + 1)\n","\n","        nn.init.normal_(self.cls_score.weight, std=0.01)\n","        nn.init.normal_(self.bbox_pred.weight, std=0.001)\n","        for item in [self.cls_score, self.bbox_pred]:\n","            nn.init.constant_(item.bias, 0)\n","\n","    def forward(self, roi_features):\n","        if roi_features.dim() > 2:\n","            roi_features = torch.flatten(roi_features, start_dim=1)\n","        scores = self.cls_score(roi_features)\n","        proposal_deltas = self.bbox_pred(roi_features)\n","        if self.use_attr:\n","            _, max_class = scores.max(-1)  # [b, c] --> [b]\n","            cls_emb = self.cls_embedding(max_class)  # [b] --> [b, 256]\n","            roi_features = torch.cat([roi_features, cls_emb], -1)  # [b, 2048] + [b, 256] --> [b, 2304]\n","            roi_features = self.fc_attr(roi_features)\n","            roi_features = nn.functional.relu(roi_features)\n","            attr_scores = self.attr_score(roi_features)\n","            return scores, attr_scores, proposal_deltas\n","        else:\n","            return scores, proposal_deltas\n","\n","\n","class GeneralizedRCNN(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","\n","        self.device = torch.device(cfg.MODEL.DEVICE)\n","        self.backbone = build_backbone(cfg)\n","        self.proposal_generator = RPN(cfg, self.backbone.output_shape())\n","        self.roi_heads = Res5ROIHeads(cfg, self.backbone.output_shape())\n","        self.roi_outputs = ROIOutputs(cfg)\n","        self.to(self.device)\n","\n","    @classmethod\n","    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n","        config = kwargs.pop(\"config\", None)\n","        state_dict = kwargs.pop(\"state_dict\", None)\n","        cache_dir = kwargs.pop(\"cache_dir\", None)\n","        from_tf = kwargs.pop(\"from_tf\", False)\n","        force_download = kwargs.pop(\"force_download\", False)\n","        resume_download = kwargs.pop(\"resume_download\", False)\n","        proxies = kwargs.pop(\"proxies\", None)\n","        local_files_only = kwargs.pop(\"local_files_only\", False)\n","        use_cdn = kwargs.pop(\"use_cdn\", True)\n","\n","        # Load config if we don't provide a configuration\n","        if not isinstance(config, Config):\n","            config_path = config if config is not None else pretrained_model_name_or_path\n","            # try:\n","            config = Config.from_pretrained(\n","                config_path,\n","                cache_dir=cache_dir,\n","                force_download=force_download,\n","                resume_download=resume_download,\n","                proxies=proxies,\n","                local_files_only=local_files_only,\n","            )\n","\n","        # Load model\n","        if pretrained_model_name_or_path is not None:\n","            if os.path.isdir(pretrained_model_name_or_path):\n","                if os.path.isfile(os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)):\n","                    # Load from a PyTorch checkpoint\n","                    archive_file = os.path.join(pretrained_model_name_or_path, WEIGHTS_NAME)\n","                else:\n","                    raise EnvironmentError(\n","                        \"Error no file named {} found in directory {} \".format(\n","                            WEIGHTS_NAME,\n","                            pretrained_model_name_or_path,\n","                        )\n","                    )\n","            elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n","                archive_file = pretrained_model_name_or_path\n","            elif os.path.isfile(pretrained_model_name_or_path + \".index\"):\n","                assert (\n","                    from_tf\n","                ), \"We found a TensorFlow checkpoint at {}, please set from_tf to True to load from this checkpoint\".format(\n","                    pretrained_model_name_or_path + \".index\"\n","                )\n","                archive_file = pretrained_model_name_or_path + \".index\"\n","            else:\n","                archive_file = hf_bucket_url(\n","                    pretrained_model_name_or_path,\n","                    filename=WEIGHTS_NAME,\n","                    use_cdn=use_cdn,\n","                )\n","\n","            try:\n","                # Load from URL or cache if already cached\n","                resolved_archive_file = cached_path(\n","                    archive_file,\n","                    cache_dir=cache_dir,\n","                    force_download=force_download,\n","                    proxies=proxies,\n","                    resume_download=resume_download,\n","                    local_files_only=local_files_only,\n","                )\n","                if resolved_archive_file is None:\n","                    raise EnvironmentError\n","            except EnvironmentError:\n","                msg = f\"Can't load weights for '{pretrained_model_name_or_path}'.\"\n","                raise EnvironmentError(msg)\n","\n","            if resolved_archive_file == archive_file:\n","                print(\"loading weights file {}\".format(archive_file))\n","            else:\n","                print(\"loading weights file {} from cache at {}\".format(archive_file, resolved_archive_file))\n","        else:\n","            resolved_archive_file = None\n","\n","        # Instantiate model.\n","        model = cls(config)\n","\n","        if state_dict is None:\n","            try:\n","                try:\n","                    state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")\n","                except Exception:\n","                    state_dict = load_checkpoint(resolved_archive_file)\n","\n","            except Exception:\n","                raise OSError(\n","                    \"Unable to load weights from pytorch checkpoint file. \"\n","                    \"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"\n","                )\n","\n","        missing_keys = []\n","        unexpected_keys = []\n","        error_msgs = []\n","\n","        # Convert old format to new format if needed from a PyTorch state_dict\n","        old_keys = []\n","        new_keys = []\n","        for key in state_dict.keys():\n","            new_key = None\n","            if \"gamma\" in key:\n","                new_key = key.replace(\"gamma\", \"weight\")\n","            if \"beta\" in key:\n","                new_key = key.replace(\"beta\", \"bias\")\n","            if new_key:\n","                old_keys.append(key)\n","                new_keys.append(new_key)\n","        for old_key, new_key in zip(old_keys, new_keys):\n","            state_dict[new_key] = state_dict.pop(old_key)\n","\n","        # copy state_dict so _load_from_state_dict can modify it\n","        metadata = getattr(state_dict, \"_metadata\", None)\n","        state_dict = state_dict.copy()\n","        if metadata is not None:\n","            state_dict._metadata = metadata\n","\n","        model_to_load = model\n","        model_to_load.load_state_dict(state_dict)\n","\n","        if model.__class__.__name__ != model_to_load.__class__.__name__:\n","            base_model_state_dict = model_to_load.state_dict().keys()\n","            head_model_state_dict_without_base_prefix = [\n","                key.split(cls.base_model_prefix + \".\")[-1] for key in model.state_dict().keys()\n","            ]\n","            missing_keys.extend(head_model_state_dict_without_base_prefix - base_model_state_dict)\n","\n","        if len(unexpected_keys) > 0:\n","            print(\n","                f\"Some weights of the model checkpoint at {pretrained_model_name_or_path} were not used when \"\n","                f\"initializing {model.__class__.__name__}: {unexpected_keys}\\n\"\n","                f\"- This IS expected if you are initializing {model.__class__.__name__} from the checkpoint of a model trained on another task \"\n","                f\"or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\\n\"\n","                f\"- This IS NOT expected if you are initializing {model.__class__.__name__} from the checkpoint of a model that you expect \"\n","                f\"to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n","            )\n","        else:\n","            print(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n","        if len(missing_keys) > 0:\n","            print(\n","                f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at {pretrained_model_name_or_path} \"\n","                f\"and are newly initialized: {missing_keys}\\n\"\n","                f\"You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n","            )\n","        else:\n","            print(\n","                f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at {pretrained_model_name_or_path}.\\n\"\n","                f\"If your task is similar to the task the model of the checkpoint was trained on, \"\n","                f\"you can already use {model.__class__.__name__} for predictions without further training.\"\n","            )\n","        if len(error_msgs) > 0:\n","            raise RuntimeError(\n","                \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n","                    model.__class__.__name__, \"\\n\\t\".join(error_msgs)\n","                )\n","            )\n","        # Set model in evaluation mode to deactivate DropOut modules by default\n","        model.eval()\n","\n","        return model\n","\n","    def forward(\n","        self,\n","        images,\n","        image_shapes,\n","        gt_boxes=None,\n","        proposals=None,\n","        scales_yx=None,\n","        **kwargs,\n","    ):\n","        \"\"\"\n","        kwargs:\n","            max_detections (int), return_tensors {\"np\", \"pt\", None}, padding {None,\n","            \"max_detections\"}, pad_value (int), location = {\"cuda\", \"cpu\"}\n","        \"\"\"\n","        if self.training:\n","            raise NotImplementedError()\n","        return self.inference(\n","            images=images,\n","            image_shapes=image_shapes,\n","            gt_boxes=gt_boxes,\n","            proposals=proposals,\n","            scales_yx=scales_yx,\n","            **kwargs,\n","        )\n","\n","    @torch.no_grad()\n","    def inference(\n","        self,\n","        images,\n","        image_shapes,\n","        gt_boxes=None,\n","        proposals=None,\n","        scales_yx=None,\n","        **kwargs,\n","    ):\n","        # run images through backbone\n","        original_sizes = image_shapes * scales_yx\n","        features = self.backbone(images)\n","\n","        # generate proposals if none are available\n","        if proposals is None:\n","            proposal_boxes, _ = self.proposal_generator(images, image_shapes, features, gt_boxes)\n","        else:\n","            assert proposals is not None\n","\n","        # pool object features from either gt_boxes, or from proposals\n","        obj_logits, attr_logits, box_deltas, feature_pooled = self.roi_heads(features, proposal_boxes, gt_boxes)\n","\n","        # prepare FRCNN Outputs and select top proposals\n","        boxes, classes, class_probs, attrs, attr_probs, roi_features = self.roi_outputs(\n","            obj_logits=obj_logits,\n","            attr_logits=attr_logits,\n","            box_deltas=box_deltas,\n","            pred_boxes=proposal_boxes,\n","            features=feature_pooled,\n","            sizes=image_shapes,\n","            scales=scales_yx,\n","        )\n","\n","        # will we pad???\n","        subset_kwargs = {\n","            \"max_detections\": kwargs.get(\"max_detections\", None),\n","            \"return_tensors\": kwargs.get(\"return_tensors\", None),\n","            \"pad_value\": kwargs.get(\"pad_value\", 0),\n","            \"padding\": kwargs.get(\"padding\", None),\n","        }\n","        preds_per_image = torch.tensor([p.size(0) for p in boxes])\n","        boxes = pad_list_tensors(boxes, preds_per_image, **subset_kwargs)\n","        classes = pad_list_tensors(classes, preds_per_image, **subset_kwargs)\n","        class_probs = pad_list_tensors(class_probs, preds_per_image, **subset_kwargs)\n","        attrs = pad_list_tensors(attrs, preds_per_image, **subset_kwargs)\n","        attr_probs = pad_list_tensors(attr_probs, preds_per_image, **subset_kwargs)\n","        roi_features = pad_list_tensors(roi_features, preds_per_image, **subset_kwargs)\n","        subset_kwargs[\"padding\"] = None\n","        preds_per_image = pad_list_tensors(preds_per_image, None, **subset_kwargs)\n","        sizes = pad_list_tensors(image_shapes, None, **subset_kwargs)\n","        normalized_boxes = norm_box(boxes, original_sizes)\n","        return OrderedDict(\n","            {\n","                \"obj_ids\": classes,\n","                \"obj_probs\": class_probs,\n","                \"attr_ids\": attrs,\n","                \"attr_probs\": attr_probs,\n","                \"boxes\": boxes,\n","                \"sizes\": sizes,\n","                \"preds_per_image\": preds_per_image,\n","                \"roi_features\": roi_features,\n","                \"normalized_boxes\": normalized_boxes,\n","            }\n","        )"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3238,"status":"ok","timestamp":1643299424391,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"pybV-cF_1XBP"},"outputs":[],"source":["import pandas as pd\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW\n","import re\n","import math\n","import itertools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y1-pP40V1fIp"},"outputs":[],"source":["img_folder='./TRAINING/'\n","train_df=pd.read_csv('train.csv',sep='\\t')\n","val_df=pd.read_csv('test.csv',sep='\\t')\n","MAX_LEN=50\n","EPOCHS = 10\n","LEARNING_RATE = 2e-5\n","TRAIN_BATCH_SIZE = 32\n","VALID_BATCH_SIZE = 32"]},{"cell_type":"code","source":["train_df=pd.concat([train_df,val_df])\n","train_df.reset_index(inplace=True)\n","train_df.rename(columns={'Text Transcription': 'text'}, inplace=True)"],"metadata":{"id":"sWeFmKfYzZDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_folder='./test_mami/'\n","test_df=pd.read_csv('./test_mami/Test.csv',sep='\\t')\n","test_df.rename(columns={'Text Transcription': 'text'}, inplace=True)\n"],"metadata":{"id":"X7269y6LrX3o","executionInfo":{"status":"ok","timestamp":1643299444718,"user_tz":-330,"elapsed":18593,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dj2mR3Z21riX"},"outputs":[],"source":["############text preprocessingg"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":967,"status":"ok","timestamp":1643299445679,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"qQCCE6wy13yJ"},"outputs":[],"source":["from nltk.corpus import stopwords\n","from sklearn import metrics"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":436,"status":"ok","timestamp":1643299446108,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"scGUFFwU130f","outputId":"d1070dd3-2393-4639-ce70-66e706641105"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":15}],"source":["import nltk\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1643299446109,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"nWGdlXAc134u"},"outputs":[],"source":["STOPWORDS = set(stopwords.words('english'))\n","REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n","BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n","EMAIL = re.compile('^([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})$')\n","# NUMBERS = re.compile(['0-9'])\n","STOPWORDS = set(stopwords.words('english'))\n","\n","def clean_text(text):\n","  \"\"\"\n","      text: a string\n","      \n","      return: modified initial string\n","  \"\"\"\n","  text = text.lower()\n","  text = EMAIL.sub('', text)\n","  text = REPLACE_BY_SPACE_RE.sub(' ',text)\n","  text = BAD_SYMBOLS_RE.sub('',text)    \n","  text = text.replace('x','')\n","  text = ' '.join(word for word in text.split() if word not in STOPWORDS)\n","  \n","  return text"]},{"cell_type":"code","source":["train_df['text'] = train_df['text'].apply(clean_text)\n","#val_df['text'] =val_df['text'].apply(clean_text)"],"metadata":{"id":"OSDU1IXXTjaG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"id":"SHNfshKnEKgz","executionInfo":{"status":"ok","timestamp":1643299446111,"user_tz":-330,"elapsed":15,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"outputs":[],"source":["test_df['text'] = test_df['text'].apply(clean_text)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1643299446112,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"5A45J_m3137l"},"outputs":[],"source":["target_cols=['shaming','stereotype','objectification','violence']"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1643299446113,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"AmzUxEB_pCtv"},"outputs":[],"source":["\n","from transformers import VisualBertModel, BertTokenizerFast"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":368,"referenced_widgets":["bdd77bd2408844dabb0d4e9c8de65f08","ea71d4dcce604500bb25472ed2f1f995","632c3b2047e44f298a12bca4d09c4608","6a529cb12e8c4c5b81dfabb1b2bf0ea1","212792eb3d6a4c4d968eb0d7564d8465","f053e4ece1f843769895079c8c4b8963","6832171f16614cc99c6920427bf3a561","5bf9044e9955441099c6f7decc7a4a24","3ae63190e8c44399936070107d0128b6","24eccba4cc4d4261b8d6828fee1b3225","79b1fe620c1345e3a39b8a4a9259430f","4479cd96461a4d24b310b0927453404e","f00f31117a764b31ab71b68275014fed","05a25d58d76c4881be25aa756b7c4236","e4473a1b30424e0a8a578828025525e2","3aff3bec206149918d85352014879143","390503f8edf24855bc3948c427e36a68","90677d31f3384f78b11d1d51f9b19a23","c9914092108544beb4c0bce6140e459e","d5cddef1ab3c41beae224191aba889f1","9095e8c395f34672a394ba55f601e8f5","93e7a1489b884523bf8c9f2a149cc4a7","09bb86602fc44f5bb2719656721fc49b","20708bef9dd44943891a71feb4223b15","7541b5a3a0c34cd78f2edce49cdaad6a","0e61f1b32c3b4ce18dbf2ca3f8173715","9e49bf2093484243b02d97a4c8c9c5e8","fe048de6e89c4ce7842addd5ecbbefd1","3e55eb0b9d8e48bab8d9a595cc9f0b59","61c61933b23f4e51a0a697ac4d91be71","2552a45995764270a2602ecb1c405ed1","bbd8bfac05c444d4b7390a235a7ff75b","a85d7698de244bdaa389020d8ce8e59a","59929056a23b4e76af31889a56d25b33","a2fb50db54d242ee950229a706a8a4e2","0bd7e5c1236d4029b6c3690fdabc5d76","c54f1cd6de974202b4d918ee8d3dab6f","ba8a316405bb4b119bc11d98676f08a4","b5e11b62dfaa41dca1fab7f2596c4e63","c96c8686cec64a77a85dfb38bb0fad80","fca8ff4362cd459c8ee73eabc98575ac","9adfa520daeb414ca1855f4388ec6804","e454936a21c04e95afea28b4a7148296","b0ea87462d0b468f86b8f21ddb1cb0ea","73e4c0673ed641fdb3c435ed8c551522","5ff0465244f34fe0bb77bdfdf5cb4f24","83b40bb3212842fc8169e5dcbb0a13f0","2f3b0ed563b8462e958b2070a756373a","cad8bfce857a4985b68b804bacc93efe","4d1a1b5963f840dab2d394580c94073c","211c701e0de648979231e9562c9e47f0","862cd0e72ff44c30b16399bc0ef38cb5","cc0b1e4705b245aab1ca79113c0755a3","1a2a1b2ee21448a5bebfc99a504238c2","8f908f29c96249fe82837a3d71b8589e","bc6fe9c99a8848d089f40a4bdde7abd8","8f1294548c1a4f29961fbedbebb5e583","5839bb917a344da7bc21cadf2b0cab18","2430d06345af43ed8b9909275d74a3ad","fda4c8eed9bc462ea62d59ca9f4fbfcc","f1a96e284fd0423b98ba9bf7733b3738","d552d63e87f24440a65e84dd144baeb5","7b1c0abf62c0457e8776e3dfbfc023ad","bfd9388da45c4722a717c22f12c04c26","3859f23f2b9144b7906b1ea5993d5827","a6dbf043b00f4b4b96b9ac23ec53a830"]},"executionInfo":{"elapsed":17621,"status":"ok","timestamp":1643299463721,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"ee7tS-remf3q","outputId":"fbac12ff-68b9-48f9-ea25-cbacac0fd29a"},"outputs":[{"output_type":"stream","name":"stdout","text":["%s not found in cache or force_download set to True, downloading to %s https://s3.amazonaws.com/models.huggingface.co/bert/unc-nlp/frcnn-vg-finetuned/config.yaml /root/.cache/torch/transformers/tmpkyw0cfno\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bdd77bd2408844dabb0d4e9c8de65f08","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/2.13k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["loading configuration file cache\n","%s not found in cache or force_download set to True, downloading to %s https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin /root/.cache/torch/transformers/tmpj_jgrc7o\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4479cd96461a4d24b310b0927453404e","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/262M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /root/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n","All model checkpoint weights were used when initializing GeneralizedRCNN.\n","\n","All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09bb86602fc44f5bb2719656721fc49b","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59929056a23b4e76af31889a56d25b33","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"73e4c0673ed641fdb3c435ed8c551522","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bc6fe9c99a8848d089f40a4bdde7abd8","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{}}],"source":["# load models and model components\n","frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n","\n","frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n","\n","image_preprocess = Preprocess(frcnn_cfg)\n","\n","bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n","#visualbert_vqa = VisualBertForQuestionAnswering.from_pretrained(\"uclanlp/visualbert-vqa\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmL56KpvtHhu"},"outputs":[],"source":["######### instead of url, see to give image paths or what"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Gf3TUyvviy6"},"outputs":[],"source":["class VisualBertDataset(Dataset):\n","  def __init__(self,df,img_folder,tokenizer,max_len):\n","    self.df=df\n","    self.max_len=max_len\n","    self.text=df.text\n","    self.tokenizer=tokenizer\n","    self.targets = df[target_cols].values\n","    self.images_id=df.file_name\n","\n","  def __len__(self):\n","        return len(self.df)\n","\n","  def __getitem__(self, index):\n","        text = self.text[index]\n","        img_name=self.images_id[index]\n","        images, sizes, scales_yx = image_preprocess(img_folder+img_name)\n","        output_dict = frcnn(images,sizes,scales_yx=scales_yx,padding=\"max_detections\",max_detections=frcnn_cfg.max_detections,return_tensors=\"pt\")\n","        visual_embeds = output_dict.get(\"roi_features\")\n","        visual_embeds=visual_embeds.squeeze(0)\n","        #print(visual_embeds.size())\n","        tokens=self.tokenizer(text,padding='max_length',max_length=MAX_LEN, add_special_tokens=True,truncation=True,return_tensors=\"pt\")\n","\n","        ids = tokens['input_ids'].squeeze(0)\n","        #print(ids.size())\n","        mask = tokens['attention_mask'].squeeze(0)\n","        token_type_ids = tokens[\"token_type_ids\"].squeeze(0)\n","        #visual_embeds = torch.stack(visual_embeds)\n","        \n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'targets': torch.tensor(self.targets[index], dtype=torch.float),\n","            'visual_embeds':visual_embeds,\n","            'visual_attention_mask': torch.ones(visual_embeds.shape[:-1], dtype=torch.long),\n","            'visual_token_type_ids':torch.ones(visual_embeds.shape[:-1], dtype=torch.long)}"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"f9aHxDuIEkk_","executionInfo":{"status":"ok","timestamp":1643299463721,"user_tz":-330,"elapsed":14,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"outputs":[],"source":["class VisualBertDatasetest(Dataset):\n","  def __init__(self,df,img_folder,tokenizer,max_len):\n","    self.df=df\n","    self.max_len=max_len\n","    self.text=df.text\n","    self.tokenizer=tokenizer\n","    self.images_id=df.file_name\n","\n","  def __len__(self):\n","        return len(self.df)\n","\n","  def __getitem__(self, index):\n","        text = self.text[index]\n","        img_name=self.images_id[index]\n","        images, sizes, scales_yx = image_preprocess(img_folder+img_name)\n","        output_dict = frcnn(images,sizes,scales_yx=scales_yx,padding=\"max_detections\",max_detections=frcnn_cfg.max_detections,return_tensors=\"pt\")\n","        visual_embeds = output_dict.get(\"roi_features\")\n","        visual_embeds=visual_embeds.squeeze(0)\n","        #print(visual_embeds.size())\n","        tokens=self.tokenizer(text,padding='max_length',max_length=MAX_LEN, add_special_tokens=True,truncation=True,return_tensors=\"pt\")\n","\n","        ids = tokens['input_ids'].squeeze(0)\n","        #print(ids.size())\n","        mask = tokens['attention_mask'].squeeze(0)\n","        token_type_ids = tokens[\"token_type_ids\"].squeeze(0)\n","        #visual_embeds = torch.stack(visual_embeds)\n","        \n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(mask, dtype=torch.long),\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n","            'visual_embeds':visual_embeds,\n","            'visual_attention_mask': torch.ones(visual_embeds.shape[:-1], dtype=torch.long),\n","            'visual_token_type_ids':torch.ones(visual_embeds.shape[:-1], dtype=torch.long)}"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"E27Ror0Pwr_x","executionInfo":{"status":"ok","timestamp":1643299463722,"user_tz":-330,"elapsed":14,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"outputs":[],"source":["VALID_BATCH_SIZE = 128\n","MAX_LEN=50"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2240,"status":"error","timestamp":1643281485819,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"jwDjttMtx6ZM","colab":{"base_uri":"https://localhost:8080/","height":182},"outputId":"8c2a72cc-5ec0-4292-d7d1-6d8d82f89809"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-56-41f6d323f21b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisualBertDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"]}],"source":["train_dataset = VisualBertDataset(train_df,img_folder,bert_tokenizer, MAX_LEN)\n","train_loader = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=2, shuffle=True, pin_memory=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bFbjB4rSzW9t"},"outputs":[],"source":["#valid_loader = DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=2, shuffle=False, pin_memory=True)\n","#valid_dataset = VisualBertDataset(val_df,img_folder,bert_tokenizer, MAX_LEN)"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"IPkUsHGuFC5R","executionInfo":{"status":"ok","timestamp":1643299463722,"user_tz":-330,"elapsed":14,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"outputs":[],"source":["test_dataset=VisualBertDatasetest(test_df,img_folder,bert_tokenizer, MAX_LEN)\n","test_loader = DataLoader(test_dataset, batch_size=128, num_workers=2, shuffle=False, pin_memory=True)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1643299464407,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"O1yGBwZUzW_c"},"outputs":[],"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1643299464409,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"j555PPEfzXDL"},"outputs":[],"source":["class Vb(torch.nn.Module):\n","  def __init__(self):\n","    super(Vb,self).__init__()\n","    self.visualbert=VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")\n","    self.l2 = torch.nn.Dropout(0.3)\n","    self.fc = torch.nn.Linear(768,4)\n","  def forward(self,ids,mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids):\n","    _,outputs = self.visualbert(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids, visual_embeds=visual_embeds, visual_attention_mask=visual_attention_mask, visual_token_type_ids=visual_token_type_ids,return_dict=False)\n","    outputs=self.l2(outputs)\n","    print(outputs.shape)\n","    outputs=self.fc(outputs)\n","    return outputs\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["fe74c1636f4145d3ad8e808c9e59c5e3","84fc5a9e1a1443e2b2af47238d4d3e4f","5913dace9cde447395ead4e6771d7171","79bc1ed06bee4776b46ef1a162dc6e47","055b42e2413947b1922b70fa40b8b64c","51cee75640dd4d3eb7fb9ca47a216011","5a8f8ed3023b4703a6c5bef097f8eb89","a7de129f5ea54bb7aa78cc256d00176d","44a291a77c82449f81c573cbf7a1e0b1","249781fb7ecc41bab7d9c22b111f76db","c0b074115d0748138c7393bac3f1a01e","15143aecc071478bab0c73da6cd4a104","4948c4eff23d4176993ae89223f75bae","cc9246e93d1f46169d08cb9942491804","5d74bf5fb6d84fe3b08ebc53a740fc46","8b20d79c6504433c94fcd9cd5b93207b","d798b1f78ead49c889274cd29541e80e","9b904eb6378146dfaa1ff0f76562705e","2ad5261505f54af79afa38e98c4761c1","c95e1139b857410d8d868470b46885e1","20fcfded81b646778847ee166dae2c10","eb5da9c23d7a42bb9e530ac95de9381c"]},"executionInfo":{"elapsed":15267,"status":"ok","timestamp":1643299479656,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"},"user_tz":-330},"id":"GNDfehKNzXFh","outputId":"327d7aba-e7a4-4a8f-ff0a-55357d89b678"},"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fe74c1636f4145d3ad8e808c9e59c5e3","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/631 [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"15143aecc071478bab0c73da6cd4a104","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/428M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias']\n","- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"execute_result","data":{"text/plain":["Vb(\n","  (visualbert): VisualBertModel(\n","    (embeddings): VisualBertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=1)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","      (visual_token_type_embeddings): Embedding(2, 768)\n","      (visual_position_embeddings): Embedding(512, 768)\n","      (visual_projection): Linear(in_features=2048, out_features=768, bias=True)\n","    )\n","    (encoder): VisualBertEncoder(\n","      (layer): ModuleList(\n","        (0): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): VisualBertLayer(\n","          (attention): VisualBertAttention(\n","            (self): VisualBertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): VisualBertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): VisualBertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): VisualBertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): VisualBertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (l2): Dropout(p=0.3, inplace=False)\n","  (fc): Linear(in_features=768, out_features=4, bias=True)\n",")"]},"metadata":{},"execution_count":26}],"source":["model=Vb()\n","model.to(device)"]},{"cell_type":"code","source":["optimizer = AdamW(params =  model.parameters(), lr=LEARNING_RATE, weight_decay=1e-6)"],"metadata":{"id":"gVVfup9LTr1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_fn(outputs, targets):\n","    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"],"metadata":{"id":"bkGpeFYrs6ri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def loss_fn(outputs, targets):\n","    return torch.nn.MultiLabelSoftMarginLoss()(outputs, targets)"],"metadata":{"id":"Y1YuJqox5_SV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def focal_binary_cross_entropy(logits, targets, gamma=2):\n","    num_label=7\n","    l = logits.reshape(-1)\n","    t = targets.reshape(-1)\n","    p = torch.sigmoid(l)\n","    p = torch.where(t >= 0.5, p, 1-p)\n","    logp = - torch.log(torch.clamp(p, 1e-4, 1-1e-4))\n","    loss = logp*((1-p)**gamma)\n","    loss = num_label*loss.mean()\n","    return loss"],"metadata":{"id":"KivpiLofcuD9"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wt1LKswixxcb"},"outputs":[],"source":["map_location=torch.device('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_EW8hc0rJWa"},"outputs":[],"source":["state = torch.load('./saved_models/visualbert_50_dp_10_fulln_focal.pt',map_location=torch.device('cpu'))\n","\n","model.load_state_dict(state['state_dict'])\n","optimizer.load_state_dict(state['optimizer'])"]},{"cell_type":"code","source":["#test\n","state = torch.load('./saved_models/visualbert_50_dp_10_fulln_focal.pt',map_location=torch.device('cpu'))\n","model.load_state_dict(state['state_dict'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1SWDMPQ1yyHY","executionInfo":{"status":"ok","timestamp":1643299499070,"user_tz":-330,"elapsed":17936,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}},"outputId":"18630929-c020-4cd0-8646-eee98bb44e22"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kq5euvMCzXM_"},"outputs":[],"source":["def train(epoch):\n","    model.train()\n","    for _, data in enumerate(train_loader, 0):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        targets = data['targets'].to(device, dtype = torch.float)\n","        visual_embeds=data['visual_embeds'].to(device)\n","        visual_attention_mask=data['visual_attention_mask'].to(device,dtype=torch.long)\n","        visual_token_type_ids=data['visual_token_type_ids'].to(device,dtype=torch.long)\n","\n","        outputs = model(ids,mask,token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids)\n","\n","        loss = focal_binary_cross_entropy(outputs, targets)\n","        if _% 10 == 0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","            state={'epoch': epoch, 'state_dict':model.state_dict(),'optimizer': optimizer.state_dict()}\n","            torch.save(state, './saved_models/visualbert_50_dp_10_fulln_focal.pt')\n","        \n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uk00xl5f0xq2","outputId":"702d239c-d206-41bf-9ada-161d33930af8"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([32, 768])\n","Epoch: 0, Loss:  0.5134871602058411\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","Epoch: 0, Loss:  0.6099765300750732\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","Epoch: 0, Loss:  0.36259162425994873\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","Epoch: 0, Loss:  0.4236854910850525\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","torch.Size([32, 768])\n","Epoch: 0, Loss:  0.7607118487358093\n","torch.Size([32, 768])\n"]}],"source":["for epoch in range(EPOCHS):\n","    train(epoch)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjETWn-0GiMm"},"outputs":[],"source":["from sklearn import metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XoIhMQgR0xtu"},"outputs":[],"source":["def validation():\n","    model.eval()\n","    fin_targets=[]\n","    fin_outputs=[]\n","    with torch.no_grad():\n","        for _, data in enumerate(valid_loader, 0):\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            targets = data['targets'].to(device, dtype = torch.float)\n","            visual_embeds=data['visual_embeds'].to(device)\n","            visual_attention_mask=data['visual_attention_mask'].to(device,dtype=torch.long)\n","            visual_token_type_ids=data['visual_token_type_ids'].to(device,dtype=torch.long)\n","            outputs = model(ids, mask, token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids)\n","            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n","            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","    return fin_outputs, fin_targets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iaI20txsDZaP","outputId":"e4ee3abb-1eee-4c59-8ab7-02f1408fa6bd"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"name":"stdout","output_type":"stream","text":["torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n"]}],"source":["outputs, targets = validation()\n","outputs = np.array(outputs) >= 0.5\n","accuracy = metrics.accuracy_score(targets, outputs)\n","f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n","f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n","print(f\"Accuracy Score = {accuracy}\")\n","print(f\"F1 Score (Micro) = {f1_score_micro}\")\n","print(f\"F1 Score (Macro) = {f1_score_macro}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LiGwkS1IDZdH"},"outputs":[],"source":["state={'epoch': epoch, 'state_dict':model.state_dict(),'optimizer': optimizer.state_dict()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPBjJKF1Di2h"},"outputs":[],"source":["torch.save(state, './saved_models/visualbert_50_dp_10.pt')"]},{"cell_type":"code","source":["####################### Predict#######"],"metadata":{"id":"mP24WYviwYWB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test():\n","    model.eval()\n","    fin_outputs=[]\n","    with torch.no_grad():\n","        for _, data in enumerate(test_loader, 0):\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            visual_embeds=data['visual_embeds'].to(device)\n","            visual_attention_mask=data['visual_attention_mask'].to(device,dtype=torch.long)\n","            visual_token_type_ids=data['visual_token_type_ids'].to(device,dtype=torch.long)\n","            outputs = model(ids, mask, token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids)\n","            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n","    return fin_outputs"],"metadata":{"id":"qWgDhXc1wYX3","executionInfo":{"status":"ok","timestamp":1643299499072,"user_tz":-330,"elapsed":11,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def test_soft():\n","    model.eval()\n","    fin_outputs=[]\n","    with torch.no_grad():\n","      for _, data in enumerate(test_loader, 0):\n","        ids = data['ids'].to(device, dtype = torch.long)\n","        mask = data['mask'].to(device, dtype = torch.long)\n","        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","        visual_embeds=data['visual_embeds'].to(device)\n","        visual_attention_mask=data['visual_attention_mask'].to(device,dtype=torch.long)\n","        visual_token_type_ids=data['visual_token_type_ids'].to(device,dtype=torch.long)\n","        outputs = model(ids, mask, token_type_ids,visual_embeds,visual_attention_mask,visual_token_type_ids)\n","        fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n","    return fin_outputs"],"metadata":{"id":"-aTNqtSJw43m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs=test()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q7pkh_92wYcr","outputId":"ed1daea8-f9cd-47b0-c5b8-1e1fdb8aa83a","executionInfo":{"status":"ok","timestamp":1643315762336,"user_tz":-330,"elapsed":16262814,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([104, 768])\n"]}]},{"cell_type":"code","source":["outputs=np.array(outputs)"],"metadata":{"id":"8heDTv9BAqnA","executionInfo":{"status":"ok","timestamp":1643317802601,"user_tz":-330,"elapsed":356,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AjOBd5VFObOp","executionInfo":{"status":"ok","timestamp":1643317806704,"user_tz":-330,"elapsed":364,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}},"outputId":"9bfc2488-63e6-4196-97c1-6595aea84042"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.40310141, 0.58415401, 0.39937404, 0.26472533],\n","       [0.4847669 , 0.70323694, 0.44293267, 0.34463644],\n","       [0.17221606, 0.4027096 , 0.2880494 , 0.2225327 ],\n","       ...,\n","       [0.46074495, 0.46630889, 0.57597047, 0.33039337],\n","       [0.20387813, 0.44841456, 0.32402739, 0.20465691],\n","       [0.35790884, 0.36853749, 0.499951  , 0.2892434 ]])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["np.savez_compressed('visualbertfull_focal.npz',outputs)"],"metadata":{"id":"4hyXVuqUAgip","executionInfo":{"status":"ok","timestamp":1643317822812,"user_tz":-330,"elapsed":373,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["outputs1 = np.array(outputs) >= 0.5\n","outputs1=np.round(outputs1)"],"metadata":{"id":"WaucRHrqUAHB","executionInfo":{"status":"ok","timestamp":1643317867155,"user_tz":-330,"elapsed":376,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["outputs1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hKNsS_yWyUVH","executionInfo":{"status":"ok","timestamp":1643317869759,"user_tz":-330,"elapsed":370,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}},"outputId":"9ace8b5a-4763-4bee-c97d-2c1fba3e20ca"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 0., 0.],\n","       [0., 1., 0., 0.],\n","       [0., 0., 0., 0.],\n","       ...,\n","       [0., 0., 1., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.]], dtype=float16)"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["outputs=test_soft()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EEygj6c-wYfi","executionInfo":{"status":"ok","timestamp":1642855021437,"user_tz":-330,"elapsed":13684453,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}},"outputId":"2ac77f07-f196-48c6-d15b-4b2d9fd1fe95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([128, 768])\n","torch.Size([104, 768])\n"]}]},{"cell_type":"code","source":["outputs = np.clip(np.sign(np.array(outputs)), a_min=0,a_max=None)\n","outputs=np.round(outputs)"],"metadata":{"id":"qkHg8V7_ziNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2HyuBbSCynaD","executionInfo":{"status":"ok","timestamp":1642855286703,"user_tz":-330,"elapsed":29,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}},"outputId":"7fcd7327-23bc-44da-8081-bc402ccb4b06"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.]])"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["predictions_db = pd.DataFrame(outputs1,  columns=['shaming','stereotype','objectification','violence'])\n","predictions_db = predictions_db.apply(lambda x:  list(map(int,x)))\n","predictions_db['file_name']=test_df['file_name']"],"metadata":{"id":"JPvwlxJJxCGr","executionInfo":{"status":"ok","timestamp":1643317879041,"user_tz":-330,"elapsed":384,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["for i in predictions_db.index:\n","  if predictions_db.loc[i,'shaming']==1 or predictions_db.loc[i,'stereotype']==1 or predictions_db.loc[i,'objectification']==1 or predictions_db.loc[i,'violence']==1:\n","    predictions_db.loc[i,'misogynous']=1\n","  else:\n","    predictions_db.loc[i,'misogynous']=0\n","predictions_db = predictions_db[['file_name','misogynous', 'shaming','stereotype','objectification','violence']]\n","predictions_db['misogynous']=predictions_db['misogynous'].apply(int)"],"metadata":{"id":"8CKcbOGKxCIi","executionInfo":{"status":"ok","timestamp":1643317879857,"user_tz":-330,"elapsed":452,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["predictions_db.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"pFkOhYm3xCNA","executionInfo":{"status":"ok","timestamp":1643317879857,"user_tz":-330,"elapsed":4,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}},"outputId":"0f9a6de2-60b5-4dfa-abc7-d62fc5166458"},"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-867351fd-48ca-4a7e-bf7c-4218cd162b08\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>file_name</th>\n","      <th>misogynous</th>\n","      <th>shaming</th>\n","      <th>stereotype</th>\n","      <th>objectification</th>\n","      <th>violence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>15236.jpg</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>15805.jpg</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>16254.jpg</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>16191.jpg</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>15952.jpg</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-867351fd-48ca-4a7e-bf7c-4218cd162b08')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-867351fd-48ca-4a7e-bf7c-4218cd162b08 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-867351fd-48ca-4a7e-bf7c-4218cd162b08');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["   file_name  misogynous  shaming  stereotype  objectification  violence\n","0  15236.jpg           1        0           1                0         0\n","1  15805.jpg           1        0           1                0         0\n","2  16254.jpg           0        0           0                0         0\n","3  16191.jpg           1        0           1                0         0\n","4  15952.jpg           1        0           1                0         0"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["predictions_db.to_csv('./final_res/answer7.txt', index=False, sep='\\t', header=False)"],"metadata":{"id":"KmQkqgTuxCQw","executionInfo":{"status":"ok","timestamp":1643317888365,"user_tz":-330,"elapsed":380,"user":{"displayName":"pandrangi aditya 19250016","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06638018349544797631"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"nOuliF0kydDY"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"visual_bert.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"bdd77bd2408844dabb0d4e9c8de65f08":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ea71d4dcce604500bb25472ed2f1f995","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_632c3b2047e44f298a12bca4d09c4608","IPY_MODEL_6a529cb12e8c4c5b81dfabb1b2bf0ea1","IPY_MODEL_212792eb3d6a4c4d968eb0d7564d8465"]}},"ea71d4dcce604500bb25472ed2f1f995":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"632c3b2047e44f298a12bca4d09c4608":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f053e4ece1f843769895079c8c4b8963","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6832171f16614cc99c6920427bf3a561"}},"6a529cb12e8c4c5b81dfabb1b2bf0ea1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5bf9044e9955441099c6f7decc7a4a24","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2132,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2132,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3ae63190e8c44399936070107d0128b6"}},"212792eb3d6a4c4d968eb0d7564d8465":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_24eccba4cc4d4261b8d6828fee1b3225","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2.13k/2.13k [00:00&lt;00:00, 34.1kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_79b1fe620c1345e3a39b8a4a9259430f"}},"f053e4ece1f843769895079c8c4b8963":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6832171f16614cc99c6920427bf3a561":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5bf9044e9955441099c6f7decc7a4a24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"3ae63190e8c44399936070107d0128b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"24eccba4cc4d4261b8d6828fee1b3225":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"79b1fe620c1345e3a39b8a4a9259430f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4479cd96461a4d24b310b0927453404e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f00f31117a764b31ab71b68275014fed","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_05a25d58d76c4881be25aa756b7c4236","IPY_MODEL_e4473a1b30424e0a8a578828025525e2","IPY_MODEL_3aff3bec206149918d85352014879143"]}},"f00f31117a764b31ab71b68275014fed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"05a25d58d76c4881be25aa756b7c4236":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_390503f8edf24855bc3948c427e36a68","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_90677d31f3384f78b11d1d51f9b19a23"}},"e4473a1b30424e0a8a578828025525e2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c9914092108544beb4c0bce6140e459e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":262398754,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":262398754,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d5cddef1ab3c41beae224191aba889f1"}},"3aff3bec206149918d85352014879143":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9095e8c395f34672a394ba55f601e8f5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 262M/262M [00:12&lt;00:00, 38.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_93e7a1489b884523bf8c9f2a149cc4a7"}},"390503f8edf24855bc3948c427e36a68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"90677d31f3384f78b11d1d51f9b19a23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c9914092108544beb4c0bce6140e459e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d5cddef1ab3c41beae224191aba889f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9095e8c395f34672a394ba55f601e8f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"93e7a1489b884523bf8c9f2a149cc4a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"09bb86602fc44f5bb2719656721fc49b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_20708bef9dd44943891a71feb4223b15","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7541b5a3a0c34cd78f2edce49cdaad6a","IPY_MODEL_0e61f1b32c3b4ce18dbf2ca3f8173715","IPY_MODEL_9e49bf2093484243b02d97a4c8c9c5e8"]}},"20708bef9dd44943891a71feb4223b15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7541b5a3a0c34cd78f2edce49cdaad6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fe048de6e89c4ce7842addd5ecbbefd1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3e55eb0b9d8e48bab8d9a595cc9f0b59"}},"0e61f1b32c3b4ce18dbf2ca3f8173715":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_61c61933b23f4e51a0a697ac4d91be71","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2552a45995764270a2602ecb1c405ed1"}},"9e49bf2093484243b02d97a4c8c9c5e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bbd8bfac05c444d4b7390a235a7ff75b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 226k/226k [00:00&lt;00:00, 1.60MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a85d7698de244bdaa389020d8ce8e59a"}},"fe048de6e89c4ce7842addd5ecbbefd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3e55eb0b9d8e48bab8d9a595cc9f0b59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"61c61933b23f4e51a0a697ac4d91be71":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2552a45995764270a2602ecb1c405ed1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bbd8bfac05c444d4b7390a235a7ff75b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a85d7698de244bdaa389020d8ce8e59a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"59929056a23b4e76af31889a56d25b33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a2fb50db54d242ee950229a706a8a4e2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0bd7e5c1236d4029b6c3690fdabc5d76","IPY_MODEL_c54f1cd6de974202b4d918ee8d3dab6f","IPY_MODEL_ba8a316405bb4b119bc11d98676f08a4"]}},"a2fb50db54d242ee950229a706a8a4e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0bd7e5c1236d4029b6c3690fdabc5d76":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b5e11b62dfaa41dca1fab7f2596c4e63","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c96c8686cec64a77a85dfb38bb0fad80"}},"c54f1cd6de974202b4d918ee8d3dab6f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fca8ff4362cd459c8ee73eabc98575ac","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":466062,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":466062,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9adfa520daeb414ca1855f4388ec6804"}},"ba8a316405bb4b119bc11d98676f08a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e454936a21c04e95afea28b4a7148296","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 455k/455k [00:00&lt;00:00, 1.15MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b0ea87462d0b468f86b8f21ddb1cb0ea"}},"b5e11b62dfaa41dca1fab7f2596c4e63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c96c8686cec64a77a85dfb38bb0fad80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fca8ff4362cd459c8ee73eabc98575ac":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9adfa520daeb414ca1855f4388ec6804":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e454936a21c04e95afea28b4a7148296":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b0ea87462d0b468f86b8f21ddb1cb0ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"73e4c0673ed641fdb3c435ed8c551522":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5ff0465244f34fe0bb77bdfdf5cb4f24","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_83b40bb3212842fc8169e5dcbb0a13f0","IPY_MODEL_2f3b0ed563b8462e958b2070a756373a","IPY_MODEL_cad8bfce857a4985b68b804bacc93efe"]}},"5ff0465244f34fe0bb77bdfdf5cb4f24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"83b40bb3212842fc8169e5dcbb0a13f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_4d1a1b5963f840dab2d394580c94073c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_211c701e0de648979231e9562c9e47f0"}},"2f3b0ed563b8462e958b2070a756373a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_862cd0e72ff44c30b16399bc0ef38cb5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cc0b1e4705b245aab1ca79113c0755a3"}},"cad8bfce857a4985b68b804bacc93efe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1a2a1b2ee21448a5bebfc99a504238c2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28.0/28.0 [00:00&lt;00:00, 651B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8f908f29c96249fe82837a3d71b8589e"}},"4d1a1b5963f840dab2d394580c94073c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"211c701e0de648979231e9562c9e47f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"862cd0e72ff44c30b16399bc0ef38cb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cc0b1e4705b245aab1ca79113c0755a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1a2a1b2ee21448a5bebfc99a504238c2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8f908f29c96249fe82837a3d71b8589e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bc6fe9c99a8848d089f40a4bdde7abd8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8f1294548c1a4f29961fbedbebb5e583","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5839bb917a344da7bc21cadf2b0cab18","IPY_MODEL_2430d06345af43ed8b9909275d74a3ad","IPY_MODEL_fda4c8eed9bc462ea62d59ca9f4fbfcc"]}},"8f1294548c1a4f29961fbedbebb5e583":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5839bb917a344da7bc21cadf2b0cab18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f1a96e284fd0423b98ba9bf7733b3738","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d552d63e87f24440a65e84dd144baeb5"}},"2430d06345af43ed8b9909275d74a3ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7b1c0abf62c0457e8776e3dfbfc023ad","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":570,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":570,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bfd9388da45c4722a717c22f12c04c26"}},"fda4c8eed9bc462ea62d59ca9f4fbfcc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3859f23f2b9144b7906b1ea5993d5827","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 570/570 [00:00&lt;00:00, 8.80kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a6dbf043b00f4b4b96b9ac23ec53a830"}},"f1a96e284fd0423b98ba9bf7733b3738":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d552d63e87f24440a65e84dd144baeb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b1c0abf62c0457e8776e3dfbfc023ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bfd9388da45c4722a717c22f12c04c26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3859f23f2b9144b7906b1ea5993d5827":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a6dbf043b00f4b4b96b9ac23ec53a830":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fe74c1636f4145d3ad8e808c9e59c5e3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_84fc5a9e1a1443e2b2af47238d4d3e4f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5913dace9cde447395ead4e6771d7171","IPY_MODEL_79bc1ed06bee4776b46ef1a162dc6e47","IPY_MODEL_055b42e2413947b1922b70fa40b8b64c"]}},"84fc5a9e1a1443e2b2af47238d4d3e4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5913dace9cde447395ead4e6771d7171":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_51cee75640dd4d3eb7fb9ca47a216011","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5a8f8ed3023b4703a6c5bef097f8eb89"}},"79bc1ed06bee4776b46ef1a162dc6e47":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a7de129f5ea54bb7aa78cc256d00176d","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":631,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":631,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_44a291a77c82449f81c573cbf7a1e0b1"}},"055b42e2413947b1922b70fa40b8b64c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_249781fb7ecc41bab7d9c22b111f76db","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 631/631 [00:00&lt;00:00, 11.8kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c0b074115d0748138c7393bac3f1a01e"}},"51cee75640dd4d3eb7fb9ca47a216011":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5a8f8ed3023b4703a6c5bef097f8eb89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a7de129f5ea54bb7aa78cc256d00176d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"44a291a77c82449f81c573cbf7a1e0b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"249781fb7ecc41bab7d9c22b111f76db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c0b074115d0748138c7393bac3f1a01e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"15143aecc071478bab0c73da6cd4a104":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4948c4eff23d4176993ae89223f75bae","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cc9246e93d1f46169d08cb9942491804","IPY_MODEL_5d74bf5fb6d84fe3b08ebc53a740fc46","IPY_MODEL_8b20d79c6504433c94fcd9cd5b93207b"]}},"4948c4eff23d4176993ae89223f75bae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cc9246e93d1f46169d08cb9942491804":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d798b1f78ead49c889274cd29541e80e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9b904eb6378146dfaa1ff0f76562705e"}},"5d74bf5fb6d84fe3b08ebc53a740fc46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2ad5261505f54af79afa38e98c4761c1","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":448356189,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":448356189,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c95e1139b857410d8d868470b46885e1"}},"8b20d79c6504433c94fcd9cd5b93207b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_20fcfded81b646778847ee166dae2c10","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 428M/428M [00:12&lt;00:00, 42.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_eb5da9c23d7a42bb9e530ac95de9381c"}},"d798b1f78ead49c889274cd29541e80e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9b904eb6378146dfaa1ff0f76562705e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2ad5261505f54af79afa38e98c4761c1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c95e1139b857410d8d868470b46885e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"20fcfded81b646778847ee166dae2c10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"eb5da9c23d7a42bb9e530ac95de9381c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}